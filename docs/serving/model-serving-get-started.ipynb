{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(model-serving-get-started)=\n",
    "# Model serving graph\n",
    "\n",
    "\n",
    "**In this section**\n",
    "- [Serving Functions](#serving-functions)\n",
    "- [Topology](#topology)\n",
    "- [Remote execution](#remote-execution)\n",
    "- [Examples of graph functionality](#examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Serving Functions\n",
    "\n",
    "To start using a serving graph, you first need a serving function. A serving function contains the serving\n",
    "class code to run the model and all the code necessary to run the tasks. MLRun comes with a wide library of tasks. If you\n",
    "use just those, you don't have to add any special code to the serving function, you just have to provide\n",
    "the code that runs the model. For more information about serving classes see {ref}`custom-model-serving-class`.\n",
    "\n",
    "For example, the following code is a basic model serving class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "source": [
    "# mlrun: start-code"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "source": [
    "from cloudpickle import load\n",
    "from typing import List\n",
    "import numpy as np\n",
    "\n",
    "import mlrun\n",
    "\n",
    "\n",
    "class ClassifierModel(mlrun.serving.V2ModelServer):\n",
    "    def load(self):\n",
    "        \"\"\"load and initialize the model and/or other elements\"\"\"\n",
    "        model_file, extra_data = self.get_model(\".pkl\")\n",
    "        self.model = load(open(model_file, \"rb\"))\n",
    "\n",
    "    def predict(self, body: dict) -> List:\n",
    "        \"\"\"Generate model predictions from sample.\"\"\"\n",
    "        feats = np.asarray(body[\"inputs\"])\n",
    "        result: np.ndarray = self.model.predict(feats)\n",
    "        return result.tolist()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "source": [
    "# mlrun: end-code"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To obtain the serving function, use the `code_to_function` and specify `kind` to be `serving`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "source": [
    "fn = mlrun.code_to_function(\"serving_example\", kind=\"serving\", image=\"mlrun/mlrun\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Router\n",
    "Once you have a serving function, you need to choose the graph topology. The default is `router` topology. With the `router` topology you can specify different machine learning models. Each model has a logical name. This name is used to route to the correct model when calling the serving function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "source": [
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# set the topology/router\n",
    "graph = fn.set_topology(\"router\")\n",
    "\n",
    "# Add the model\n",
    "fn.add_model(\n",
    "    \"model1\",\n",
    "    class_name=\"ClassifierModel\",\n",
    "    model_path=\"https://s3.wasabisys.com/iguazio/models/iris/model.pkl\",\n",
    ")\n",
    "\n",
    "# Add additional models\n",
    "# fn.add_model(\"model2\", class_name=\"ClassifierModel\", model_path=\"<path2>\")\n",
    "\n",
    "# create and use the graph simulator\n",
    "server = fn.to_mock_server()\n",
    "x = load_iris()[\"data\"].tolist()\n",
    "result = server.test(\"/v2/models/model1/infer\", {\"inputs\": x})\n",
    "server.wait_for_completion()\n",
    "\n",
    "print(result)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flow\n",
    "\n",
    "You can use the `flow` topology to specify tasks, which typically manipulates the data. The most common scenario is pre-processing of data prior to the model execution.\n",
    "\n",
    "```{note} Once the topology is set, you cannot change an existing function topology.\n",
    "```\n",
    "\n",
    "In this topology, you build and connect the graph (DAG) by adding steps using the `step.to()` method, or by using the \n",
    "`graph.add_step()` method.\n",
    "\n",
    "> The `step.to()` is typically used to chain steps together. `graph.add_step` can add steps anywhere on the\n",
    "> graph and has `before` and `after` parameters to specify the location of the step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- show example without router -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "source": [
    "fn2 = mlrun.code_to_function(\n",
    "    \"serving_example_flow\", kind=\"serving\", image=\"mlrun/mlrun\"\n",
    ")\n",
    "\n",
    "graph2 = fn2.set_topology(\"flow\")\n",
    "\n",
    "graph2_enrich = graph2.to(\"storey.Extend\", name=\"enrich\", _fn='({\"tag\": \"something\"})')\n",
    "\n",
    "# add an Ensemble router with two child models (routes)\n",
    "router = graph2.add_step(mlrun.serving.ModelRouter(), name=\"router\", after=\"enrich\")\n",
    "router.add_route(\n",
    "    \"m1\",\n",
    "    class_name=\"ClassifierModel\",\n",
    "    model_path=\"https://s3.wasabisys.com/iguazio/models/iris/model.pkl\",\n",
    ")\n",
    "router.respond()\n",
    "\n",
    "# add an error handling step, run only when/if the \"pre-process\" step fails\n",
    "graph.to(name=\"pre-process\", handler=\"raising_step\").error_handler(\n",
    "    name=\"catcher\", handler=\"handle_error\", full_event=True\n",
    ")\n",
    "\n",
    "# Add additional models\n",
    "# router.add_route(\"m2\", class_name=\"ClassifierModel\", model_path=path2)\n",
    "\n",
    "# plot the graph (using Graphviz)\n",
    "graph2.plot(rankdir=\"LR\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "source": [
    "fn2_server = fn2.to_mock_server()\n",
    "\n",
    "result = fn2_server.test(\"/v2/models/m1/infer\", {\"inputs\": x})\n",
    "fn2_server.wait_for_completion()\n",
    "\n",
    "print(result)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remote execution\n",
    "\n",
    "You can chain functions together with remote execution. This allows you to:\n",
    "- Call existing functions from the graph and reuse them from other graphs.\n",
    "- Scale up and down different components individually.\n",
    "\n",
    "Calling a remote function can either use HTTP or via a queue (streaming)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HTTP\n",
    "\n",
    "Calling a function using http uses the special `$remote` class. First deploy the remote function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "source": [
    "remote_func_name = \"serving-example-flow\"\n",
    "project_name = \"graph-basic-concepts\"\n",
    "fn_remote = mlrun.code_to_function(\n",
    "    remote_func_name, project=project_name, kind=\"serving\", image=\"mlrun/mlrun\"\n",
    ")\n",
    "\n",
    "fn_remote.add_model(\n",
    "    \"model1\",\n",
    "    class_name=\"ClassifierModel\",\n",
    "    model_path=\"https://s3.wasabisys.com/iguazio/models/iris/model.pkl\",\n",
    ")\n",
    "\n",
    "remote_addr = fn_remote.deploy()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a new function with a graph and call the remote function above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "source": [
    "fn_preprocess = mlrun.new_function(\"preprocess\", kind=\"serving\")\n",
    "graph_preprocessing = fn_preprocess.set_topology(\"flow\")\n",
    "\n",
    "graph_preprocessing.to(\"storey.Extend\", name=\"enrich\", _fn='({\"tag\": \"something\"})').to(\n",
    "    \"$remote\", \"remote_func\", url=f\"{remote_addr}v2/models/model1/infer\", method=\"put\"\n",
    ").respond()\n",
    "\n",
    "graph_preprocessing.plot(rankdir=\"LR\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "source": [
    "fn3_server = fn_preprocess.to_mock_server()\n",
    "my_data = \"\"\"{\"inputs\":[[5.1, 3.5, 1.4, 0.2],[7.7, 3.8, 6.7, 2.2]]}\"\"\"\n",
    "result = fn3_server.test(\"/v2/models/my_model/infer\", body=my_data)\n",
    "fn3_server.wait_for_completion()\n",
    "print(result)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Queue (streaming)\n",
    "\n",
    "You can use queues to send events from one part of the graph to another and to decouple the processing of those parts.\n",
    "Queues are better suited to deal with bursts of events, since all the events are stored in the queue until they are processed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(serving-v3io-stream-example)=\n",
    "#### V3IO stream example\n",
    "The example below uses a V3IO stream, which is a fast real-time implementation of a stream that allows processing of events at very low latency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "source": [
    "%%writefile echo.py\n",
    "def echo_handler(x):\n",
    "    print(x)\n",
    "    return x"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configure the streams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "source": [
    "import os\n",
    "\n",
    "streams_prefix = (\n",
    "    f\"v3io:///users/{os.getenv('V3IO_USERNAME')}/examples/graph-basic-concepts\"\n",
    ")\n",
    "\n",
    "input_stream = streams_prefix + \"/in-stream\"\n",
    "out_stream = streams_prefix + \"/out-stream\"\n",
    "err_stream = streams_prefix + \"/err-stream\""
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternativey, use Kafka to configure the streams:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "kafka_prefix = f\"kafka://{broker}/\"\n",
    "internal_topic = kafka_prefix + \"in-topic\"\n",
    "out_topic = kafka_prefix + \"out-topic\"\n",
    "err_topic = kafka_prefix + \"err-topic\""
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the graph. In the `to` method the class name is one of `>>` or `$queue` to specify that this is a queue. To configure a consumer group for the step, include the group in the `to` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "source": [
    "fn_preprocess2 = mlrun.new_function(\"preprocess\", kind=\"serving\")\n",
    "fn_preprocess2.add_child_function(\"echo_func\", \"./echo.py\", \"mlrun/mlrun\")\n",
    "\n",
    "graph_preprocess2 = fn_preprocess2.set_topology(\"flow\")\n",
    "\n",
    "graph_preprocess2.to(\"storey.Extend\", name=\"enrich\", _fn='({\"tag\": \"something\"})').to(\n",
    "    \">>\", \"input_stream\", path=input_stream, group=\"mygroup\"\n",
    ").to(name=\"echo\", handler=\"echo_handler\", function=\"echo_func\").to(\n",
    "    \">>\", \"output_stream\", path=out_stream, sharding_func=\"partition\"\n",
    ")\n",
    "\n",
    "graph_preprocess2.plot(rankdir=\"LR\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "source": [
    "from echo import *\n",
    "\n",
    "fn4_server = fn_preprocess2.to_mock_server(current_function=\"*\")\n",
    "\n",
    "my_data = \"\"\"{\"inputs\": [[5.1, 3.5, 1.4, 0.2], [7.7, 3.8, 6.7, 2.2]], \"partition\": 0}\"\"\"\n",
    "\n",
    "result = fn4_server.test(\"/v2/models/my_model/infer\", body=my_data)\n",
    "fn4_server.wait_for_completion()\n",
    "\n",
    "print(result)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(serving-kafka-stream-example)=\n",
    "#### Kafka stream example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "source": [
    "%%writefile echo.py\n",
    "def echo_handler(x):\n",
    "    print(x)\n",
    "    return x"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configure the streams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import os\n",
    "\n",
    "input_topic = \"in-topic\"\n",
    "out_topic = \"out-topic\"\n",
    "err_topic = \"err-topic\"\n",
    "\n",
    "# replace this\n",
    "brokers = \"<broker IP>\""
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the graph. In the `to` method the class name is one of `>>` or `$queue` to specify that this is a queue. To configure a consumer group for the step, include the group in the `to` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import mlrun\n",
    "\n",
    "fn_preprocess2 = mlrun.new_function(\"preprocess\", kind=\"serving\")\n",
    "fn_preprocess2.add_child_function(\"echo_func\", \"./echo.py\", \"mlrun/mlrun\")\n",
    "\n",
    "graph_preprocess2 = fn_preprocess2.set_topology(\"flow\")\n",
    "\n",
    "graph_preprocess2.to(\"storey.Extend\", name=\"enrich\", _fn='({\"tag\": \"something\"})').to(\n",
    "    \">>\",\n",
    "    \"input_stream\",\n",
    "    path=input_topic,\n",
    "    group=\"mygroup\",\n",
    "    kafka_brokers=brokers,\n",
    ").to(name=\"echo\", handler=\"echo_handler\", function=\"echo_func\").to(\n",
    "    \">>\", \"output_stream\", path=out_topic, kafka_brokers=brokers\n",
    ")\n",
    "\n",
    "graph_preprocess2.plot(rankdir=\"LR\")\n",
    "\n",
    "from echo import *\n",
    "\n",
    "fn4_server = fn_preprocess2.to_mock_server(current_function=\"*\")\n",
    "\n",
    "fn4_server.set_error_stream(f\"kafka://{brokers}/{err_topic}\")\n",
    "\n",
    "my_data = \"\"\"{\"inputs\":[[5.1, 3.5, 1.4, 0.2],[7.7, 3.8, 6.7, 2.2]]}\"\"\"\n",
    "\n",
    "result = fn4_server.test(\"/v2/models/my_model/infer\", body=my_data)\n",
    "fn4_server.wait_for_completion()\n",
    "\n",
    "print(result)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples of graph functionality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLP processing pipeline with real-time streaming \n",
    "\n",
    "In some cases it's useful to split your processing to multiple functions and use \n",
    "streaming protocols to connect those functions. \n",
    "\n",
    "See the [full notebook example](./distributed-graph.html), where the data processing is in the first function/container and the NLP processing is in the second function. And the second function contains the GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Currently queues support Iguazio v3io and Kafka streams."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph that splits and rejoins\n",
    "\n",
    "You can define a graph that splits into two parallel steps, and the output of both steps join back together. \n",
    "\n",
    "In this basic example, all input goes into both stepA and stepB, and then both stepA and stepB forward the input to stepC. \n",
    "This means that a dataset of 5 rows generates an output of 10 rows (barring any filtering or other processing that \n",
    "would change the number of rows).\n",
    "\n",
    "```{admonition} Note\n",
    "Use this configuration to join the graph branches and **not** to join the events into a single large one.\n",
    "``` \n",
    "\n",
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "graph.to(\"stepB\")\n",
    "graph.to(\"stepC\")\n",
    "graph.add_step(name=\"stepD\", after=[\"stepB\", \"stepC\"])\n",
    "\n",
    "\n",
    "graph = fn.set_topology(\"flow\", exist_ok=True)\n",
    "dbl = graph.to(name=\"double\", handler=\"double\")\n",
    "dbl.to(name=\"add3\", class_name=\"Adder\", add=3)\n",
    "dbl.to(name=\"add2\", class_name=\"Adder\", add=2)\n",
    "graph.add_step(\"Gather\").after(\"add2\", \"add3\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Graphs that split and rejoin can also be used for these types of scenarios:\n",
    "- Steps B and C are filter steps that complement each other. For example B passes events where key < X, and C passes events where key >= X. The resulting DF contains the exact event ingested, since each event was handled once on one of the branches.\n",
    "- Steps B and C modify the content of the event in different ways. B adds a column col1 with value X, and C adds a column col2 with value X. The resulting DF contains both col1 and col2. Each key is represented twice: once with col1 == X, col2 == null and once with col1 == null, col2 == X."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
