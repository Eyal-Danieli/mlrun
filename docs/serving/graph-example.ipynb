{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(graph-example)=\n",
    "# Advanced model serving graph - notebook example\n",
    "\n",
    "This example demonstrates how to use MLRun serving graphs and their advanced functionality including:\n",
    "* Use of flow, task, model, and ensemble router states\n",
    "* Build tasks from custom handlers, classes and storey components\n",
    "* Use custom error handlers\n",
    "* Test graphs locally\n",
    "* Deploy the graph as a real-time serverless functions\n",
    "\n",
    "**In this example**\n",
    "- [Define functions and classes used in the graph](#define-functions-and-classes-used-in-the-graph)\n",
    "- [Create a new serving function and graph](#create-a-new-serving-function-and-graph)\n",
    "- [Test the function locally](#test-the-function-locally)\n",
    "- [Deploy the graph as a real-time serverless function](#deploy-the-graph-as-a-real-time-serverless-function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define functions and classes used in the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "source": [
    "from cloudpickle import load\n",
    "from typing import List\n",
    "from sklearn.datasets import load_iris\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# model serving class example\n",
    "class ClassifierModel(mlrun.serving.V2ModelServer):\n",
    "    def load(self):\n",
    "        \"\"\"load and initialize the model and/or other elements\"\"\"\n",
    "        model_file, extra_data = self.get_model(\".pkl\")\n",
    "        self.model = load(open(model_file, \"rb\"))\n",
    "\n",
    "    def predict(self, body: dict) -> List:\n",
    "        \"\"\"Generate model predictions from sample.\"\"\"\n",
    "        feats = np.asarray(body[\"inputs\"])\n",
    "        result: np.ndarray = self.model.predict(feats)\n",
    "        return result.tolist()\n",
    "\n",
    "\n",
    "# echo class, custom class example\n",
    "class Echo:\n",
    "    def __init__(self, context, name=None, **kw):\n",
    "        self.context = context\n",
    "        self.name = name\n",
    "        self.kw = kw\n",
    "\n",
    "    def do(self, x):\n",
    "        print(\"Echo:\", self.name, x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# error echo function, demo catching error and using custom function\n",
    "def error_catcher(x):\n",
    "    x.body = {\"body\": x.body, \"origin_state\": x.origin_state, \"error\": x.error}\n",
    "    print(\"EchoError:\", x)\n",
    "    return None"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "source": [
    "# mark the end of the code section, DO NOT REMOVE !\n",
    "# mlrun: end-code"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a new serving function and graph\n",
    "Use `code_to_function` to convert the above code into a serving function object and initialize a graph with async flow topology."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "source": [
    "function = mlrun.code_to_function(\n",
    "    \"advanced\", kind=\"serving\", image=\"mlrun/mlrun\", requirements=[\"storey\"]\n",
    ")\n",
    "graph = function.set_topology(\"flow\", engine=\"async\")\n",
    "# function.verbose = True"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify the sklearn models that are used in the ensemble."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "source": [
    "models_path = \"https://s3.wasabisys.com/iguazio/models/iris/model.pkl\"\n",
    "path1 = models_path\n",
    "path2 = models_path"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build and connect the graph (DAG) using the custom function and classes and plot the result. Add states \n",
    "using the `state.to()` method (adds a new state after the current one), or using the \n",
    "`graph.add_step()` method.\n",
    "\n",
    "Use the graph `error_handler` if you want an error from the graph or a step to be fed into a specific state (catcher). See the full description in {ref}`pipelines-error-handling`.\n",
    "\n",
    "You can specify which state is the responder (returns the HTTP response) using the `state.respond()` method.\n",
    "If you don't specify the responder, the graph is non-blocking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "source": [
    "# use built-in storey class or our custom Echo class to create and link Task steps. Add an error handling step that runs if the grah step fails\n",
    "graph.to(\"storey.Extend\", name=\"enrich\", _fn='({\"tag\": \"something\"})').to(\n",
    "    class_name=\"Echo\", name=\"pre-process\", some_arg=\"abc\"\n",
    ").error_handler(name=\"catcher\", handler=\"handle_error\", full_event=True)\n",
    "\n",
    "# add an Ensemble router with two child models (routes). The \"*\" prefix mark it is a router class\n",
    "router = graph.add_step(\n",
    "    \"*mlrun.serving.VotingEnsemble\", name=\"ensemble\", after=\"pre-process\"\n",
    ")\n",
    "router.add_route(\"m1\", class_name=\"ClassifierModel\", model_path=path1)\n",
    "router.add_route(\"m2\", class_name=\"ClassifierModel\", model_path=path2)\n",
    "\n",
    "# add the final step (after the router) that handles post processing and responds to the client\n",
    "graph.add_step(class_name=\"Echo\", name=\"final\", after=\"ensemble\").respond()\n",
    "\n",
    "# plot the graph (using Graphviz) and run a test\n",
    "graph.plot(rankdir=\"LR\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the function locally\n",
    "Create a test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "source": [
    "import random\n",
    "\n",
    "iris = load_iris()\n",
    "x = random.sample(iris[\"data\"].tolist(), 5)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a mock server (simulator) and test the graph with the test data.\n",
    "\n",
    "> Note: The model and router objects support a common serving protocol API, see the [protocol and API section](model-api.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "source": [
    "server = function.to_mock_server()\n",
    "resp = server.test(\"/v2/models/infer\", body={\"inputs\": x})\n",
    "server.wait_for_completion()\n",
    "resp"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy the graph as a real-time serverless function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "source": [
    "function.deploy()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Invoke the remote function using the test data</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "source": [
    "function.invoke(\"/v2/models/infer\", body={\"inputs\": x})"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [],
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
