{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4: Automated ML pipeline\n",
    "\n",
    "MLRun Project is a container for all your work on a particular activity: all of the associated code, functions, \n",
    "jobs/workflows and artifacts. Projects can be mapped to `git` repositories, which enable versioning, collaboration, and CI/CD.\n",
    "Users can create project definitions using the SDK or a yaml file and store those in MLRun DB, file, or archive.\n",
    "Once the project is loaded you can run jobs/workflows that refer to any project element by name, allowing separation between configuration and code. \n",
    "\n",
    "Projects contain `workflows` that execute the registered functions in a sequence/graph (DAG), can reference project \n",
    "parameters, secrets and artifacts by name. This notebook demonstrates how to build an automated workflow with \n",
    "**feature selection**, **training**, **testing**, and **deployment**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"gs-tutorial-4-prerequisites\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Setting up your project\n",
    "\n",
    "To run a pipeline, you first need to get or create a project object and define/import the required functions for its execution.\n",
    "See the [**Create, save, and use projects**](../../projects/create-project.html) for details.\n",
    "\n",
    "The following code gets or creates a user project named \"fraud-demo<username>\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "source": [
    "# Set the base project name\n",
    "project_name = \"fraud-demo\""
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "source": [
    "import mlrun\n",
    "\n",
    "# Initialize the MLRun project object\n",
    "project = mlrun.get_or_create_project(project_name, context=\"./\", user_project=True)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"gs-tutorial-4-step-import-functions\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Updating project and function definitions\n",
    "\n",
    "You need to save the definitions for the function you use in the projects. This enables automatically converting code \n",
    "to functions or import external functions whenever you load new versions of your code or when you run automated CI/CD \n",
    "workflows. In addition, you may want to set other project attributes such as global parameters, secrets, and data.\n",
    "\n",
    "Your code can be stored in Python files, notebooks, external repositories, packaged containers, etc. You use the \n",
    "`project.set_function()` method to register your code in the project. The definitions are saved to the project object, as \n",
    "well as in a YAML file in the root of our project.\n",
    "Functions can also be imported from MLRun marketplace (using the `hub://` schema).\n",
    "\n",
    "This tutorial uses these functions:\n",
    "- `feature_selection` &mdash; the first function, which determines the top features to be used for training.\n",
    "- `train` &mdash; the model-training function\n",
    "- `evaluate` &mdash; the model-testing function\n",
    "- `mlrun-model` &mdash; the model-serving function\n",
    "\n",
    "```{admonition} Note\n",
    "`set_function` uses the `code_to_function` and `import_function` methods under the hood (used in the previous notebooks), but in addition it saves the function configurations in the project spec for use in automated workflows and CI/CD. \n",
    "```\n",
    "Add the function definitions to the project along with parameters and data artifacts and save the project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"gs-tutorial-4-view-project-functions\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "source": [
    "project.set_function(\"hub://feature_selection\", \"feature_selection\")\n",
    "project.set_function(\"hub://auto_trainer\", \"train\")\n",
    "project.set_function(\"hub://v2_model_server\", \"serving\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "source": [
    "# set project level parameters and save\n",
    "project.spec.params = {\"label_column\": \"label\"}\n",
    "project.save()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>When you save the project it stores the project definitions in the `project.yaml`. This allows you to load the project \n",
    "from the source control (GIT) and run it with a single command or API call.\n",
    "\n",
    "The project YAML for this project can be printed using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "source": [
    "print(project.to_yaml())"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving and loading projects from GIT\n",
    "\n",
    "After you saved your project and its elements (functions, workflows, artifacts, etc.) you can commit all your changes to a \n",
    "GIT repository. This can be done using standard GIT tools or using MLRun `project` methods such as `pull`, `push`, \n",
    "`remote`, which calls the Git API for you.\n",
    "\n",
    "Projects can then be loaded from Git using MLRun `load_project` method, for example: \n",
    "\n",
    "    project = mlrun.load_project(\"./myproj\", \"git://github.com/mlrun/project-demo.git\", name=project_name)\n",
    "    \n",
    "or using MLRun CLI:\n",
    "\n",
    "    mlrun project -n myproj -u \"git://github.com/mlrun/project-demo.git\" ./myproj\n",
    "    \n",
    "Read [CI/CD integration](../../projects/ci-integration.html) for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"gs-tutorial-4-kubeflow-pipelines\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Kubeflow pipelines\n",
    "\n",
    "You're now ready to create a full ML pipeline.\n",
    "This is done by using [Kubeflow Pipelines](https://www.kubeflow.org/docs/pipelines/overview/pipelines-overview/) &mdash;\n",
    "an open-source framework for building and deploying portable, scalable machine-learning workflows based on Docker containers.\n",
    "MLRun leverages this framework to take your existing code and deploy it as steps in the pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Defining and saving a pipeline workflow\n",
    "\n",
    "A pipeline is created by running an MLRun **\"workflow\"**.\n",
    "The following code defines a workflow and writes it to a file in your local directory.\n",
    "(The file name is **workflow.py**.)\n",
    "The workflow describes a directed acyclic graph (DAG) for execution using Kubeflow Pipelines, and depicts the connections \n",
    "between the functions and the data as part of an end-to-end pipeline.\n",
    "The workflow file has a definition of a pipeline DSL for connecting the function inputs and outputs.\n",
    "\n",
    "The defined pipeline includes the following steps:\n",
    "\n",
    "- Perform feature selection (`feature_selection`).\n",
    "- Train and the model (`train`).\n",
    "- Test the model with its test data set (`evaluate`).\n",
    "- Deploy the model as a real-time serverless function (`deploy`).\n",
    "\n",
    "```{admonition} Note\n",
    "A pipeline can also include continuous build integration and deployment (CI/CD) steps, such as building container images and deploying models.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "source": [
    "%%writefile workflow.py\n",
    "import mlrun\n",
    "from kfp import dsl\n",
    "from mlrun.model import HyperParamOptions\n",
    "\n",
    "from mlrun import (\n",
    "    build_function,\n",
    "    deploy_function,\n",
    "    import_function,\n",
    "    run_function,\n",
    ")\n",
    "\n",
    "    \n",
    "@dsl.pipeline(\n",
    "    name=\"Fraud Detection Pipeline\",\n",
    "    description=\"Detecting fraud from a transactions dataset\"\n",
    ")\n",
    "\n",
    "def kfpipeline(vector_name='transactions-fraud'):\n",
    "    \n",
    "    project = mlrun.get_current_project()\n",
    "    \n",
    "    # Feature selection   \n",
    "    feature_selection = run_function(\n",
    "        \"feature_selection\",\n",
    "        name=\"feature_selection\",\n",
    "        params={'output_vector_name': \"short\",\n",
    "                \"label_column\": project.get_param('label_column', 'label'),\n",
    "                \"k\": 18,\n",
    "                \"min_votes\": 2,\n",
    "                'ignore_type_errors': True\n",
    "               },\n",
    "        inputs={'df_artifact': project.get_artifact_uri(vector_name, 'feature-vector')},\n",
    "        outputs=['feature_scores', 'selected_features_count', 'top_features_vector', 'selected_features'])\n",
    "    \n",
    "    \n",
    "    # train with hyper-paremeters\n",
    "    train = run_function(\n",
    "        \"train\",\n",
    "        name=\"train\",\n",
    "        handler=\"train\",\n",
    "        params={\"sample\": -1, \n",
    "                \"label_column\": project.get_param('label_column', 'label'),\n",
    "                \"test_size\": 0.10},\n",
    "        hyperparams={\"model_name\": ['transaction_fraud_rf', \n",
    "                                    'transaction_fraud_xgboost', \n",
    "                                    'transaction_fraud_adaboost'],\n",
    "                     'model_class': [\"sklearn.ensemble.RandomForestClassifier\", \n",
    "                                     \"sklearn.linear_model.LogisticRegression\",\n",
    "                                     \"sklearn.ensemble.AdaBoostClassifier\"]},\n",
    "        hyper_param_options=HyperParamOptions(selector=\"max.accuracy\"),\n",
    "        inputs={\"dataset\": feature_selection.outputs['top_features_vector']},\n",
    "        outputs=['model', 'test_set'])\n",
    "    \n",
    "            \n",
    "    # test and visualize your model\n",
    "    test = run_function(\n",
    "        \"train\",\n",
    "        name=\"evaluate\",\n",
    "        handler='evaluate',\n",
    "        params={\"label_columns\": project.get_param('label_column', 'label'),\n",
    "                \"model\": train.outputs[\"model\"], \n",
    "                \"drop_columns\": project.get_param('label_column', 'label')},\n",
    "        inputs={\"dataset\": train.outputs[\"test_set\"]})\n",
    "    \n",
    "    # route your serving model to use enrichment\n",
    "    funcs['serving'].set_topology('router', \n",
    "                                  'mlrun.serving.routers.EnrichmentModelRouter', \n",
    "                                  name='EnrichmentModelRouter', \n",
    "                                  feature_vector_uri=\"transactions-fraud-short\", \n",
    "                                  impute_policy={\"*\": \"$mean\"},\n",
    "                                  exist_ok=True)\n",
    "\n",
    "    \n",
    "    # deploy your model as a serverless function, you can pass a list of models to serve \n",
    "    deploy = deploy_function(\"serving\", models=[{\"key\": 'fraud', \"model_path\": train.outputs[\"model\"]}])"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"gs-tutorial-4-step-register-workflow\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Registering the workflow\n",
    "\n",
    "Use the `set_workflow` MLRun project method to register your workflow with MLRun.\n",
    "The following code sets the `name` parameter to the selected workflow name (\"main\") and the `code` parameter to the name of \n",
    "the workflow file that is found in your project directory (**workflow.py**)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "source": [
    "# Register the workflow file as \"main\"\n",
    "project.set_workflow(\"main\", \"workflow.py\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"gs-tutorial-4-step-run-pipeline\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Running a pipeline\n",
    "\n",
    "First run the following code to save your project:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "source": [
    "project.save()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the `run` MLRun project method to execute your workflow pipeline with Kubeflow Pipelines.\n",
    "\n",
    "You can pass **`arguments`** or set the **`artifact_path`** to specify a unique path for storing the workflow artifacts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "source": [
    "run_id = project.run(\"main\", arguments={}, dirty=True, watch=True)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"gs-tutorial-4-step-ui-pipeline-view\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Test the model endpoint\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that your model is deployed using the pipeline, you can invoke it as usual:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "source": [
    "# Define your serving function\n",
    "serving_fn = project.get_function(\"serving\")\n",
    "\n",
    "# Choose an id for your test\n",
    "sample_id = \"C1000148617\"\n",
    "model_inference_path = \"/v2/models/fraud/infer\"\n",
    "\n",
    "# Send our sample ID for predcition\n",
    "serving_fn.invoke(path=model_inference_path, body={\"inputs\": [[sample_id]]})"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"gs-tutorial-4-done\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Done!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
