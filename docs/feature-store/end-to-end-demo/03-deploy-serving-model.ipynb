{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Serving\n",
    "\n",
    "In this part you use MLRun's **serving runtime** to deploy your trained models from the previous stage, a `Voting Ensemble` using **max vote** logic.\n",
    "You also use MLRun's **Feature store** to receive the latest tag of the online **Feature Vector** you defined in the previous stage.\n",
    "\n",
    "By the end of this tutorial youâ€™ll learn how to:\n",
    "- Define a model class to load your models, run preprocessing, and predict on the data\n",
    "- Define a Voting Ensemble function on top of your models\n",
    "- Test the serving function locally using your `mock server`\n",
    "- Deploy the function to the cluster and test it live"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment setup\n",
    "\n",
    "First, make sure SciKit-Learn is installed in the correct version:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "!pip install -U scikit-learn"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Restart your kernel post installing.\n",
    "Since your work is done in this project scope, you should define the project itself for all your MLRun work in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "source": [
    "project_name = \"fraud-demo\""
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "source": [
    "import mlrun\n",
    "\n",
    "# Initialize the MLRun project object\n",
    "project = mlrun.get_or_create_project(project_name, context=\"./\", user_project=True)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define model class\n",
    "- Load models\n",
    "- Predict from the feature store online service via the `source` key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "source": [
    "# mlrun: start-code"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "from cloudpickle import load\n",
    "from mlrun.serving.v2_serving import V2ModelServer\n",
    "\n",
    "\n",
    "class ClassifierModel(V2ModelServer):\n",
    "    def load(self):\n",
    "        \"\"\"load and initialize the model and/or other elements\"\"\"\n",
    "        model_file, extra_data = self.get_model(\".pkl\")\n",
    "        self.model = load(open(model_file, \"rb\"))\n",
    "\n",
    "    def predict(self, body: dict) -> list:\n",
    "        \"\"\"Generate model predictions from sample\"\"\"\n",
    "        print(f\"Input -> {body['inputs']}\")\n",
    "        feats = np.asarray(body[\"inputs\"])\n",
    "        result: np.ndarray = self.model.predict(feats)\n",
    "        return result.tolist()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "source": [
    "# mlrun: end-code"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a serving function\n",
    "\n",
    "MLRun serving can produce managed real-time serverless pipelines from various tasks, including MLRun models or standard model files.\n",
    "The pipelines use the Nuclio real-time serverless engine, which can be deployed anywhere.\n",
    "[Nuclio](https://nuclio.io/) is a high-performance open-source serverless framework that's focused on data, I/O, and compute-intensive workloads.\n",
    "\n",
    "The **EnrichmentVotingEnsemble** and the **EnrichmentModelRouter** router classes auto-enrich the request with data from the feature store.\n",
    "The router input accepts a list of inference requests (each request can be a dict or list of incoming features/keys). It enriches the request with data from the specified feature vector (`feature_vector_uri`).\n",
    "\n",
    "In many cases the features can have null values (None, NaN, Inf, ..). The `Enrichment` routers can substitute the null \n",
    "value with fixed or statistical value per feature. This is done through the `impute_policy` parameter, which accepts the \n",
    "impute policy per feature (where `*` is used to specify the default). The value can be a fixed number for constants or \n",
    "`$mean`, `$max`, `$min`, `$std`, `$count` for statistical values, to substitute the value with the equivalent feature stats (taken from the feature store).  \n",
    "\n",
    "The following code achieves:\n",
    "\n",
    "- Gather ClassifierModel code from this notebook\n",
    "- Define `EnrichmentVotingEnsemble` - Max-Vote based ensemble with feature enrichment and imputing\n",
    "- Add the previously trained models to the ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "source": [
    "# Create the serving function from your code above\n",
    "serving_fn = mlrun.code_to_function(\n",
    "    \"transaction-fraud\", kind=\"serving\", image=\"mlrun/mlrun\"\n",
    ").apply(mlrun.auto_mount())\n",
    "\n",
    "serving_fn.set_topology(\n",
    "    \"router\",\n",
    "    \"mlrun.serving.routers.EnrichmentVotingEnsemble\",\n",
    "    name=\"VotingEnsemble\",\n",
    "    feature_vector_uri=\"transactions-fraud-short\",\n",
    "    impute_policy={\"*\": \"$mean\"},\n",
    ")\n",
    "\n",
    "model_names = [\n",
    "    \"transaction_fraud_rf\",\n",
    "    \"transaction_fraud_xgboost\",\n",
    "    \"transaction_fraud_adaboost\",\n",
    "]\n",
    "\n",
    "for i, name in enumerate(model_names, start=1):\n",
    "    serving_fn.add_model(\n",
    "        name,\n",
    "        class_name=\"ClassifierModel\",\n",
    "        model_path=project.get_artifact_uri(f\"{name}#{i}:latest\"),\n",
    "    )\n",
    "\n",
    "# Plot the ensemble configuration\n",
    "serving_fn.spec.graph.plot()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the server locally\n",
    "\n",
    "Before deploying the serving function, you can test it in the current notebook and check the model output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "source": [
    "# Create a mock server from the serving function\n",
    "local_server = serving_fn.to_mock_server()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "source": [
    "# Choose an id for your test\n",
    "sample_id = \"C1000148617\"\n",
    "\n",
    "model_inference_path = \"/v2/models/infer\"\n",
    "\n",
    "# Send your sample ID for prediction\n",
    "local_server.test(path=model_inference_path, body={\"inputs\": [[sample_id]]})\n",
    "\n",
    "# notice the input vector is printed 3 times (once per child model) and is enriched with data from the feature store"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accessing the real-time feature vector directly\n",
    "\n",
    "You can also directly query the feature store values using the `get_online_feature_service` method. This method is used internally in the EnrichmentVotingEnsemble router class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "source": [
    "import mlrun.feature_store as fstore\n",
    "\n",
    "# Create the online feature service\n",
    "svc = fstore.get_online_feature_service(\n",
    "    \"transactions-fraud-short:latest\", impute_policy={\"*\": \"$mean\"}\n",
    ")\n",
    "\n",
    "# Get sample feature vector\n",
    "sample_fv = svc.get([{\"source\": sample_id}])\n",
    "sample_fv"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploying the function on the Kubernetes cluster\n",
    "\n",
    "You can now deploy the function. Once deployed, you get a function with http trigger that can be called from other locations.\n",
    "\n",
    "Model activities can be tracked into a real-time stream and time-series DB. The monitoring data\n",
    "is used to create real-time dashboards, detect drift, and analyze performance.<br>\n",
    "To monitor a deployed model, apply `set_tracking()`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "source": [
    "import os\n",
    "\n",
    "# Enable model monitoring\n",
    "serving_fn.set_tracking()\n",
    "project.set_model_monitoring_credentials(os.getenv(\"V3IO_ACCESS_KEY\"))\n",
    "\n",
    "# Deploy the serving function\n",
    "serving_fn.deploy()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the server\n",
    "\n",
    "You can test the serving function and examine the model output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "source": [
    "# Choose an id for your test\n",
    "sample_id = \"C1000148617\"\n",
    "\n",
    "model_inference_path = \"/v2/models/infer\"\n",
    "\n",
    "# Send your sample ID for prediction\n",
    "serving_fn.invoke(path=model_inference_path, body={\"inputs\": [[sample_id]]})"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also directly query the feature store values, which are used in the enrichment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulate incoming data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "source": [
    "# Load the dataset\n",
    "data = mlrun.get_dataitem(\n",
    "    \"https://s3.wasabisys.com/iguazio/data/fraud-demo-mlrun-fs-docs/data.csv\"\n",
    ").as_df()\n",
    "\n",
    "# use only first 10k\n",
    "data = data.sort_values(by=\"source\", axis=0)[:10000]\n",
    "\n",
    "# keys\n",
    "sample_ids = data[\"source\"].to_list()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "source": [
    "from random import choice, uniform\n",
    "from time import sleep\n",
    "\n",
    "# Sending random requests\n",
    "for _ in range(10):\n",
    "    data_point = choice(sample_ids)\n",
    "    try:\n",
    "        resp = serving_fn.invoke(\n",
    "            path=model_inference_path, body={\"inputs\": [[data_point]]}\n",
    "        )\n",
    "        print(resp)\n",
    "        sleep(uniform(0.2, 1.7))\n",
    "    except OSError:\n",
    "        pass"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Done!\n",
    "\n",
    "You've completed Part 3 of the deploying the serving function.\n",
    "Proceed to [Part 4](04-pipeline.html) to learn how to automate ML Pipeline.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
