{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Data ingestion\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{admonition} Note\n",
    "This demo works with the online feature store, which is currently not part of the Open Source default deployment.\n",
    "```\n",
    "This demo showcases financial fraud prevention using the MLRun feature store to define complex features that help identify \n",
    "fraud. Fraud prevention specifically is a challenge because it requires processing raw transaction and events in real-time, and \n",
    "being able to quickly respond and block transactions before they occur.\n",
    "\n",
    "To address this, you create a development pipeline and a production pipeline. Both pipelines share the same feature \n",
    "engineering and model code, but serve data very differently. Furthermore, you automate the data and model monitoring \n",
    "process, identify drift and trigger retraining in a CI/CD pipeline. This process is described in the diagram below:\n",
    "\n",
    "![Feature store demo diagram - fraud prevention](../../_static/images/feature_store_demo_diagram.png)\n",
    "\n",
    "By the end of this tutorial youâ€™ll learn how to:\n",
    "\n",
    "- Create an ingestion pipeline for each data source.\n",
    "- Define preprocessing, aggregation and validation of the pipeline.\n",
    "- Run the pipeline locally within the notebook.\n",
    "- Launch a real-time function to ingest live data.\n",
    "- Schedule a cron to run the task when needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The raw data is described as follows:\n",
    "\n",
    "| TRANSACTIONS                                                                    || &#x2551; |USER EVENTS                                                                           || \n",
    "|-----------------|----------------------------------------------------------------|----------|-----------------|----------------------------------------------------------------|\n",
    "| **age**         | age group value 0-6. Some values are marked as U for unknown   | &#x2551; | **source**      | The party/entity related to the event                          |\n",
    "| **gender**      | A character to define the gender                               | &#x2551; | **event**       | event, such as login or password change                        |\n",
    "| **zipcodeOri**  | ZIP code of the person originating the transaction             | &#x2551; | **timestamp**   | The date and time of the event                                 |\n",
    "| **zipMerchant** | ZIP code of the merchant receiving the transaction             | &#x2551; |                 |                                                                |\n",
    "| **category**    | category of the transaction (e.g., transportation, food, etc.) | &#x2551; |                 |                                                                |\n",
    "| **amount**      | the total amount of the transaction                            | &#x2551; |                 |                                                                |\n",
    "| **fraud**       | whether the transaction is fraudulent                          | &#x2551; |                 |                                                                |\n",
    "| **timestamp**   | the date and time in which the transaction took place          | &#x2551; |                 |                                                                |\n",
    "| **source**      | the ID of the party/entity performing the transaction          | &#x2551; |                 |                                                                |\n",
    "| **target**      | the ID of the party/entity receiving the transaction           | &#x2551; |                 |                                                                |\n",
    "| **device**      | the device ID used to perform the transaction                  | &#x2551; |                 |                                                                |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook introduces how to **Ingest** different data sources to the **Feature Store**.\n",
    "\n",
    "The following FeatureSets are created:\n",
    "- **Transactions**: Monetary transactions between a source and a target.\n",
    "- **Events**: Account events such as account login or a password change.\n",
    "- **Label**: Fraud label for the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "source": [
    "!/User/align_mlrun.sh"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "source": [
    "project_name = \"fraud-demo\""
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "source": [
    "import mlrun\n",
    "\n",
    "# Initialize the MLRun project object\n",
    "project = mlrun.get_or_create_project(project_name, context=\"./\", user_project=True)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 - Fetch, process and ingest the datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 - Transactions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "source": [
    "# Helper functions to adjust the timestamps of our data\n",
    "# while keeping the order of the selected events and\n",
    "# the relative distance from one event to the other\n",
    "\n",
    "\n",
    "def date_adjustment(sample, data_max, new_max, old_data_period, new_data_period):\n",
    "    \"\"\"\n",
    "    Adjust a specific sample's date according to the original and new time periods\n",
    "    \"\"\"\n",
    "    sample_dates_scale = (data_max - sample) / old_data_period\n",
    "    sample_delta = new_data_period * sample_dates_scale\n",
    "    new_sample_ts = new_max - sample_delta\n",
    "    return new_sample_ts\n",
    "\n",
    "\n",
    "def adjust_data_timespan(\n",
    "    dataframe, timestamp_col=\"timestamp\", new_period=\"2d\", new_max_date_str=\"now\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Adjust the dataframe timestamps to the new time period\n",
    "    \"\"\"\n",
    "    # Calculate old time period\n",
    "    data_min = dataframe.timestamp.min()\n",
    "    data_max = dataframe.timestamp.max()\n",
    "    old_data_period = data_max - data_min\n",
    "\n",
    "    # Set new time period\n",
    "    new_time_period = pd.Timedelta(new_period)\n",
    "    new_max = pd.Timestamp(new_max_date_str)\n",
    "    new_min = new_max - new_time_period\n",
    "    new_data_period = new_max - new_min\n",
    "\n",
    "    # Apply the timestamp change\n",
    "    df = dataframe.copy()\n",
    "    df[timestamp_col] = df[timestamp_col].apply(\n",
    "        lambda x: date_adjustment(\n",
    "            x, data_max, new_max, old_data_period, new_data_period\n",
    "        )\n",
    "    )\n",
    "    return df"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Fetch the transactions dataset from the server\n",
    "transactions_data = pd.read_csv(\n",
    "    \"https://s3.wasabisys.com/iguazio/data/fraud-demo-mlrun-fs-docs/data.csv\",\n",
    "    parse_dates=[\"timestamp\"],\n",
    ")\n",
    "\n",
    "# use only first 50k\n",
    "transactions_data = transactions_data.sort_values(by=\"source\", axis=0)[:10000]\n",
    "\n",
    "# Adjust the samples timestamp for the past 2 days\n",
    "transactions_data = adjust_data_timespan(transactions_data, new_period=\"2d\")\n",
    "\n",
    "# Sorting after adjusting timestamps\n",
    "transactions_data = transactions_data.sort_values(by=\"timestamp\", axis=0)\n",
    "\n",
    "# Preview\n",
    "transactions_data.head(3)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transactions - create a feature set and preprocessing pipeline\n",
    "Create the feature set (data pipeline) definition for the **credit transaction processing** that describes the \n",
    "offline/online data transformations and aggregations.<br>\n",
    "The feature store automatically adds an offline `parquet` target and an online `NoSQL` target by using `set_targets()`.\n",
    "\n",
    "The data pipeline consists of:\n",
    "\n",
    "* **Extracting** the data components (hour, day of week)\n",
    "* **Mapping** the age values\n",
    "* **One hot encoding** for the transaction category and the gender\n",
    "* **Aggregating** the amount (avg, sum, count, max over 2/12/24 hour time windows)\n",
    "* **Aggregating** the transactions per category (over 14 days time windows)\n",
    "* **Writing** the results to **offline** (Parquet) and **online** (NoSQL) targets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "source": [
    "# Import MLRun's Feature Store\n",
    "import mlrun.feature_store as fstore\n",
    "from mlrun.feature_store.steps import OneHotEncoder, MapValues, DateExtractor"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "source": [
    "# Define the transactions FeatureSet\n",
    "transaction_set = fstore.FeatureSet(\n",
    "    \"transactions\",\n",
    "    entities=[fstore.Entity(\"source\")],\n",
    "    timestamp_key=\"timestamp\",\n",
    "    description=\"transactions feature set\",\n",
    ")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "source": [
    "# Define and add value mapping\n",
    "main_categories = [\n",
    "    \"es_transportation\",\n",
    "    \"es_health\",\n",
    "    \"es_otherservices\",\n",
    "    \"es_food\",\n",
    "    \"es_hotelservices\",\n",
    "    \"es_barsandrestaurants\",\n",
    "    \"es_tech\",\n",
    "    \"es_sportsandtoys\",\n",
    "    \"es_wellnessandbeauty\",\n",
    "    \"es_hyper\",\n",
    "    \"es_fashion\",\n",
    "    \"es_home\",\n",
    "    \"es_contents\",\n",
    "    \"es_travel\",\n",
    "    \"es_leisure\",\n",
    "]\n",
    "\n",
    "# One Hot Encode the newly defined mappings\n",
    "one_hot_encoder_mapping = {\n",
    "    \"category\": main_categories,\n",
    "    \"gender\": list(transactions_data.gender.unique()),\n",
    "}\n",
    "\n",
    "# Define the graph steps\n",
    "transaction_set.graph.to(\n",
    "    DateExtractor(parts=[\"hour\", \"day_of_week\"], timestamp_col=\"timestamp\")\n",
    ").to(MapValues(mapping={\"age\": {\"U\": \"0\"}}, with_original_features=True)).to(\n",
    "    OneHotEncoder(mapping=one_hot_encoder_mapping)\n",
    ")\n",
    "\n",
    "\n",
    "# Add aggregations for 2, 12, and 24 hour time windows\n",
    "transaction_set.add_aggregation(\n",
    "    name=\"amount\",\n",
    "    column=\"amount\",\n",
    "    operations=[\"avg\", \"sum\", \"count\", \"max\"],\n",
    "    windows=[\"2h\", \"12h\", \"24h\"],\n",
    "    period=\"1h\",\n",
    ")\n",
    "\n",
    "\n",
    "# Add the category aggregations over a 14 day window\n",
    "for category in main_categories:\n",
    "    transaction_set.add_aggregation(\n",
    "        name=category,\n",
    "        column=f\"category_{category}\",\n",
    "        operations=[\"sum\"],\n",
    "        windows=[\"14d\"],\n",
    "        period=\"1d\",\n",
    "    )\n",
    "\n",
    "# Add default (offline-parquet & online-nosql) targets\n",
    "transaction_set.set_targets()\n",
    "\n",
    "# Plot the pipeline so you can see the different steps\n",
    "transaction_set.plot(rankdir=\"LR\", with_targets=True)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transactions - ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "source": [
    "# Ingest your transactions dataset through your defined pipeline\n",
    "transactions_df = fstore.ingest(\n",
    "    transaction_set, transactions_data, infer_options=fstore.InferOptions.default()\n",
    ")\n",
    "\n",
    "transactions_df.head(3)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After performing the ingestion process, you can see all of the different features that were created with the help of the UI, as shown in the image below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "![Features Catalog - fraud prevention](../../_static/images/features-catalog-transaction.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 - User events"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User events - fetching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "source": [
    "# Fetch the user_events dataset from the server\n",
    "user_events_data = pd.read_csv(\n",
    "    \"https://s3.wasabisys.com/iguazio/data/fraud-demo-mlrun-fs-docs/events.csv\",\n",
    "    index_col=0,\n",
    "    quotechar=\"'\",\n",
    "    parse_dates=[\"timestamp\"],\n",
    ")\n",
    "\n",
    "# Adjust to the last 2 days to see the latest aggregations in the online feature vectors\n",
    "user_events_data = adjust_data_timespan(user_events_data, new_period=\"2d\")\n",
    "\n",
    "# Preview\n",
    "user_events_data.head(3)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User events - create a feature set and preprocessing pipeline\n",
    "\n",
    "Now define the events feature set.\n",
    "This is a pretty straightforward pipeline in which you only \"one hot encode\" the event categories and save the data to the default targets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "source": [
    "user_events_set = fstore.FeatureSet(\n",
    "    \"events\",\n",
    "    entities=[fstore.Entity(\"source\")],\n",
    "    timestamp_key=\"timestamp\",\n",
    "    description=\"user events feature set\",\n",
    ")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "source": [
    "# Define and add value mapping\n",
    "events_mapping = {\"event\": list(user_events_data.event.unique())}\n",
    "\n",
    "# One Hot Encode\n",
    "user_events_set.graph.to(OneHotEncoder(mapping=events_mapping))\n",
    "\n",
    "# Add default (offline-parquet & online-nosql) targets\n",
    "user_events_set.set_targets()\n",
    "\n",
    "# Plot the pipeline so you can see the different steps\n",
    "user_events_set.plot(rankdir=\"LR\", with_targets=True)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User Events - Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "source": [
    "# Ingestion of your newly created events feature set\n",
    "events_df = fstore.ingest(user_events_set, user_events_data)\n",
    "events_df.head(3)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 - Create a labels data set for model training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label set - create a feature set\n",
    "This feature set contains the label for the fraud demo. It is ingested directly to the default targets without any changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "source": [
    "def create_labels(df):\n",
    "    labels = df[[\"fraud\", \"timestamp\"]].copy()\n",
    "    labels = labels.rename(columns={\"fraud\": \"label\"})\n",
    "    labels[\"timestamp\"] = labels[\"timestamp\"].astype(\"datetime64[ms]\")\n",
    "    labels[\"label\"] = labels[\"label\"].astype(int)\n",
    "    return labels"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "source": [
    "from mlrun.datastore import ParquetTarget\n",
    "import os\n",
    "\n",
    "# Define the \"labels\" feature set\n",
    "labels_set = fstore.FeatureSet(\n",
    "    \"labels\",\n",
    "    entities=[fstore.Entity(\"source\")],\n",
    "    timestamp_key=\"timestamp\",\n",
    "    description=\"training labels\",\n",
    "    engine=\"pandas\",\n",
    ")\n",
    "\n",
    "labels_set.graph.to(name=\"create_labels\", handler=create_labels)\n",
    "\n",
    "\n",
    "# specify only Parquet (offline) target since its not used for real-time\n",
    "target = ParquetTarget(\n",
    "    name=\"labels\", path=f\"v3io:///projects/{project.name}/target.parquet\"\n",
    ")\n",
    "labels_set.set_targets([target], with_defaults=False)\n",
    "labels_set.plot(with_targets=True)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label set - ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "source": [
    "# Ingest the labels feature set\n",
    "labels_df = fstore.ingest(labels_set, transactions_data)\n",
    "labels_df.head(3)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3 - Deploy a real-time pipeline\n",
    "\n",
    "When dealing with real-time aggregation, it's important to be able to update these aggregations in real-time.\n",
    "For this purpose, you create live serving functions that update the online feature store of the `transactions` \n",
    "FeatureSet and `Events` FeatureSet.\n",
    "\n",
    "Using MLRun's `serving` runtime, create a nuclio function loaded with your feature set's computational graph definition\n",
    "and an `HttpSource` to define the HTTP trigger.\n",
    "\n",
    "Notice that the implementation below does not require any rewrite of the pipeline logic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 - Transactions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transactions - deploy the feature set live endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "source": [
    "# Create iguazio v3io stream and transactions push API endpoint\n",
    "transaction_stream = f\"v3io:///projects/{project.name}/streams/transaction\"\n",
    "transaction_pusher = mlrun.datastore.get_stream_pusher(transaction_stream)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "source": [
    "# Define the source stream trigger (use v3io streams)\n",
    "# define the `key` and `time` fields (extracted from the Json message).\n",
    "source = mlrun.datastore.sources.StreamSource(\n",
    "    path=transaction_stream, key_field=\"source\", time_field=\"timestamp\"\n",
    ")\n",
    "\n",
    "# Deploy the transactions feature set's ingestion service over a real-time (Nuclio) serverless function\n",
    "# you can use the run_config parameter to pass function/service specific configuration\n",
    "transaction_set_endpoint, function = transaction_set.deploy_ingestion_service(\n",
    "    source=source\n",
    ")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transactions - test the feature set HTTP endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By defining your `transactions` feature set you can now use MLRun and Storey to deploy it as a live endpoint, ready to ingest new data!\n",
    "\n",
    "Using MLRun's `serving` runtime, create a nuclio function loaded with your feature set's computational graph definition \n",
    "and an `HttpSource` to define the HTTP trigger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "# Select a sample from the dataset and serialize it to JSON\n",
    "transaction_sample = json.loads(transactions_data.sample(1).to_json(orient=\"records\"))[\n",
    "    0\n",
    "]\n",
    "transaction_sample[\"timestamp\"] = str(pd.Timestamp.now())\n",
    "transaction_sample"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "source": [
    "# Post the sample to the ingestion endpoint\n",
    "requests.post(transaction_set_endpoint, json=transaction_sample).text"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 - User events"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User events - deploy the feature set live endpoint\n",
    "Deploy the events feature set's ingestion service using the feature set and all the previously defined resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "source": [
    "# Create iguazio v3io stream and transactions push API endpoint\n",
    "events_stream = f\"v3io:///projects/{project.name}/streams/events\"\n",
    "events_pusher = mlrun.datastore.get_stream_pusher(events_stream)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Define the source stream trigger (use v3io streams)\n",
    "# define the `key` and `time` fields (extracted from the Json message).\n",
    "source = mlrun.datastore.sources.StreamSource(\n",
    "    path=events_stream, key_field=\"source\", time_field=\"timestamp\"\n",
    ")\n",
    "\n",
    "# Deploy the transactions feature set's ingestion service over a real-time (Nuclio) serverless function\n",
    "# you can use the run_config parameter to pass function/service specific configuration\n",
    "events_set_endpoint, function = user_events_set.deploy_ingestion_service(source=source)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User Events - Test the feature set HTTP endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Select a sample from the events dataset and serialize it to JSON\n",
    "user_events_sample = json.loads(user_events_data.sample(1).to_json(orient=\"records\"))[0]\n",
    "user_events_sample[\"timestamp\"] = str(pd.Timestamp.now())\n",
    "user_events_sample"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Post the sample to the ingestion endpoint\n",
    "requests.post(events_set_endpoint, json=user_events_sample).text"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Done!\n",
    "\n",
    "You've completed Part 1 of the data-ingestion with the feature store.\n",
    "Proceed to [Part 2](02-create-training-model.html) to learn how to train an ML model using the feature store data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
