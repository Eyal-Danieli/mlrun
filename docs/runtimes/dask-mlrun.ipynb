{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running Dask on the cluster with MLRun\n",
    "\n",
    "```{admonition} Note\n",
    "Dask is supported at the Tech Preview level only.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Dask framework enables you to parallelize your Python code and run it as a distributed process on an Iguazio cluster and dramatically accelerate the performance. <br>\n",
    "In this notebook you'll learn how to create a Dask cluster and then an MLRun function running as a Dask client. <br>\n",
    "It also demonstrates how to run parallelize custom algorithm using Dask Delayed option.\n",
    "\n",
    "For more information on Dask over Kubernetes: https://kubernetes.dask.org/en/latest/."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "source": [
    "# set mlrun api path and artifact path for logging\n",
    "import mlrun\n",
    "\n",
    "project = mlrun.get_or_create_project(\"dask-demo\", \"./\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create and start Dask cluster\n",
    "Dask functions can be local (local workers), or remote (use containers in the cluster). In the case of remote you \n",
    "can specify the number of replicas (optional) or leave blank for auto-scale.<br>\n",
    "Use the `new_function()` to define the Dask cluster and set the desired configuration of that clustered function.\n",
    "\n",
    "If the Dask workers need to access the shared file system, apply a shared volume mount (e.g. via v3io mount).\n",
    "\n",
    "The Dask function spec has several unique attributes (in addition to the standard job attributes):\n",
    "\n",
    "* **.remote** &mdash; bool, use local or clustered dask\n",
    "* **.replicas** &mdash; number of desired replicas, keep 0 for auto-scale\n",
    "* **.min_replicas**, **.max_replicas** &mdash; set replicas range for auto-scale\n",
    "* **.scheduler_timeout** &mdash; cluster is killed after timeout (inactivity), default is '60 minutes'\n",
    "* **.nthreads** &mdash; number of worker threads\n",
    "<br>\n",
    "\n",
    "If you want to access the Dask dashboard or scheduler from remote you need to use NodePort service type (set `.service_type` to 'NodePort'), and the external IP need to be specified in the MLRun configuration (mlconf.remote_host). This is set automatically if you are running on an Iguazio cluster.\n",
    "\n",
    "Specify the kind (dask) and the container image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "source": [
    "# create an mlrun function that will init the dask cluster\n",
    "dask_cluster_name = \"dask-cluster\"\n",
    "dask_cluster = mlrun.new_function(dask_cluster_name, kind=\"dask\", image=\"mlrun/mlrun\")\n",
    "dask_cluster.apply(mlrun.mount_v3io())"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "source": [
    "# set range for # of replicas with replicas and max_replicas\n",
    "dask_cluster.spec.min_replicas = 1\n",
    "dask_cluster.spec.max_replicas = 4\n",
    "\n",
    "# set the use of dask remote cluster (distributed)\n",
    "dask_cluster.spec.remote = True\n",
    "dask_cluster.spec.service_type = \"NodePort\"\n",
    "\n",
    "# set dask memory and cpu limits\n",
    "dask_cluster.with_worker_requests(mem=\"2G\", cpu=\"2\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize the Dask Cluster\n",
    "\n",
    "When you request the dask cluster `client` attribute, it verifies that the cluster is up and running:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "source": [
    "# init dask client and use the scheduler address as param in the following cell\n",
    "dask_cluster.client"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a function that runs over Dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "source": [
    "# mlrun: start-code"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import mlrun and dask. Nuclio is only used to convert the code into an MLRun function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "source": [
    "import mlrun"
   ],
   "outputs": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "%nuclio config kind = \"job\"\n",
    "%nuclio config spec.image = \"mlrun/ml-base\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "source": [
    "from dask.distributed import Client\n",
    "from dask import delayed\n",
    "from dask import dataframe as dd\n",
    "\n",
    "import warnings\n",
    "import numpy as np\n",
    "import os\n",
    "import mlrun\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python function code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This simple function reads a .csv file using dask dataframe. It runs the `groupby` and `describe` functions on the dataset, and stores the results as a dataset artifact.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "source": [
    "def test_dask(\n",
    "    context, dataset: mlrun.DataItem, client=None, dask_function: str = None\n",
    ") -> None:\n",
    "    # setup dask client from the MLRun dask cluster function\n",
    "    if dask_function:\n",
    "        client = mlrun.import_function(dask_function).client\n",
    "    elif not client:\n",
    "        client = Client()\n",
    "\n",
    "    # load the dataitem as dask dataframe (dd)\n",
    "    df = dataset.as_df(df_module=dd)\n",
    "\n",
    "    # run describe (get statistics for the dataframe) with dask\n",
    "    df_describe = df.describe().compute()\n",
    "\n",
    "    # run groupby and count using dask\n",
    "    df_grpby = df.groupby(\"VendorID\").count().compute()\n",
    "\n",
    "    context.log_dataset(\"describe\", df=df_grpby, format=\"csv\", index=True)\n",
    "    return"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "source": [
    "# mlrun: end-code"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the function over Dask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "source": [
    "DATA_URL = \"/User/examples/ytrip.csv\""
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "source": [
    "!mkdir -p /User/examples/\n",
    "!curl -L \"https://s3.wasabisys.com/iguazio/data/Taxi/yellow_tripdata_2019-01_subset.csv\" > {DATA_URL}"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert the code to MLRun function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `code_to_function` to convert the code to MLRun and specify the configuration for the Dask process (e.g. replicas, memory etc.). <br>\n",
    "Note that the resource configurations are per worker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "source": [
    "# mlrun transforms the code above (up to nuclio: end-code cell) into serverless function\n",
    "# which runs in k8s pods\n",
    "fn = mlrun.code_to_function(\"test_dask\", kind=\"job\", handler=\"test_dask\").apply(\n",
    "    mlrun.mount_v3io()\n",
    ")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the function"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "When running the function there is a link as part of the result. Click the link to go to the Dask monitoring dashboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "source": [
    "# function URI is db://<project>/<name>\n",
    "dask_uri = f\"db://{project.name}/{dask_cluster_name}\""
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "source": [
    "r = fn.run(\n",
    "    handler=test_dask,\n",
    "    inputs={\"dataset\": DATA_URL},\n",
    "    params={\"dask_function\": dask_uri},\n",
    "    auto_build=True,\n",
    ")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Track the progress in the UI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can view the progress and detailed information in the MLRun UI by clicking on the uid above. <br>\n",
    "To track the dask progress: in the Dask UI click the \"dashboard link\" above the \"client\" section."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
