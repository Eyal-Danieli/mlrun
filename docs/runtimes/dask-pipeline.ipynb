{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Pipelines using Dask, Kubeflow and MLRun"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Create a project to host functions, jobs and artifacts\n",
    "\n",
    "Projects are used to package multiple functions, workflows, and artifacts. Project code and definitions are usually stored in a Git archive.\n",
    "\n",
    "The following code creates a new project in a local dir and initializes git tracking on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "import os\n",
    "import mlrun\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# set project name, dir, and artifacts path\n",
    "project_name = \"sk-project-dask\"\n",
    "project_dir = \"./\"\n",
    "project.artifact_path = path\n",
    "\n",
    "# set project\n",
    "sk_dask_proj = mlrun.get_or_create_project(project_name, project_dir, init_git=True)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Init Dask cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "import mlrun\n",
    "\n",
    "# set up function from local file\n",
    "dsf = mlrun.new_function(name=\"mydask\", kind=\"dask\", image=\"mlrun/mlrun\")\n",
    "\n",
    "# set up function specs for dask\n",
    "dsf.spec.remote = True\n",
    "dsf.spec.replicas = 5\n",
    "dsf.spec.service_type = \"NodePort\"\n",
    "dsf.with_limits(mem=\"6G\")\n",
    "dsf.spec.nthreads = 5"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# apply mount_v3io over the function so that the k8s pod that runs the function\n",
    "# can access the data (shared data access)\n",
    "dsf.apply(mlrun.mount_v3io())"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "dsf.save()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# init dask cluster\n",
    "dsf.client"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Load and run a functions\n",
    "\n",
    "Load the function object from .py or .yaml file, or the Function Hub (marketplace).<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# load function from the Function Hub\n",
    "sk_dask_proj.set_function(\"hub://describe\", name=\"describe\")\n",
    "sk_dask_proj.set_function(\"hub://sklearn_classifier_dask\", name=\"dask_classifier\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Create a fully automated ML pipeline\n",
    "\n",
    "### Add more functions to the project to be used in the pipeline (from the Function Hub)\n",
    "\n",
    "Describe data, train and eval model with dask."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Define and save a pipeline\n",
    "\n",
    "The following workflow definition is written into a file. It describes a Kubeflow execution graph (DAG) \n",
    "and how functions and data are connected to form an end-to-end pipeline. \n",
    "\n",
    "* Describe data.\n",
    "* Train, test and evaluate with dask.\n",
    "\n",
    "Check the code below to see how functions objects are initialized and used (by name) inside the workflow.<br>\n",
    "The `workflow.py` file has two parts, initialize the function objects and define pipeline dsl (connect the function inputs and outputs).\n",
    "\n",
    "> Note: The pipeline can include CI steps like building container images and deploying models as illustrated in the following example.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "%%writefile workflow.py\n",
    "import os\n",
    "from kfp import dsl\n",
    "import mlrun\n",
    "\n",
    "# params\n",
    "funcs = {}\n",
    "LABELS = \"label\"\n",
    "DROP = \"congestion_surcharge\"\n",
    "DATA_URL = mlrun.get_sample_path(\"data/iris/iris_dataset.csv\")\n",
    "DASK_CLIENT = \"db://sk-project-dask/mydask\"\n",
    "\n",
    "\n",
    "# init functions are used to configure function resources and local settings\n",
    "def init_functions(functions: dict, project=None, secrets=None):\n",
    "    for f in functions.values():\n",
    "        f.apply(mlrun.mount_v3io())\n",
    "        pass\n",
    "\n",
    "\n",
    "@dsl.pipeline(name=\"Demo training pipeline\", description=\"Shows how to use mlrun\")\n",
    "def kfpipeline():\n",
    "    # Describe the data\n",
    "    describe = funcs[\"describe\"].as_step(\n",
    "        inputs={\"table\": DATA_URL},\n",
    "        params={\"dask_function\": DASK_CLIENT},\n",
    "    )\n",
    "\n",
    "    # Train, test and evaluate:\n",
    "    train = funcs[\"dask_classifier\"].as_step(\n",
    "        name=\"train\",\n",
    "        handler=\"train_model\",\n",
    "        inputs={\"dataset\": DATA_URL},\n",
    "        params={\n",
    "            \"label_column\": LABELS,\n",
    "            \"dask_function\": DASK_CLIENT,\n",
    "            \"test_size\": 0.10,\n",
    "            \"model_pkg_class\": \"sklearn.ensemble.RandomForestClassifier\",\n",
    "            \"drop_cols\": DROP,\n",
    "        },\n",
    "        outputs=[\"model\", \"test_set\"],\n",
    "    )\n",
    "    train.after(describe)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# register the workflow file as \"main\", embed the workflow code into the project YAML\n",
    "sk_dask_proj.set_workflow(\"main\", \"workflow.py\", embed=False)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Save the project definitions to a file (project.yaml). It is recommended to commit all changes to a Git repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "sk_dask_proj.save()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<a id='run-pipeline'></a>\n",
    "## Run a pipeline workflow\n",
    "Use the `run` method to execute a workflow. You can provide alternative arguments and specify the default target for workflow artifacts.<br>\n",
    "The workflow ID is returned and can be used to track the progress or you can use the hyperlinks.\n",
    "\n",
    "> Note: The same command can be issued through CLI commands:<br>\n",
    "    `mlrun project my-proj/ -r main -p \"v3io:///users/admin/mlrun/kfp/{{workflow.uid}}/\"`\n",
    "\n",
    "The `dirty` flag lets you run a project with uncommitted changes (when the notebook is in the same git dir it is always dirty).<br>\n",
    "The `watch` flag waits for the pipeline to complete and print results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "artifact_path = os.path.abspath(\"./pipe/{{workflow.uid}}\")\n",
    "run_id = sk_dask_proj.run(\n",
    "    \"main\", arguments={}, artifact_path=artifact_path, dirty=False, watch=True\n",
    ")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**[back to top](#top)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [],
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
