{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(batch_inference_overview)=\n",
    "# Batch inference\n",
    "\n",
    "Batch inference or offline inference addresses the need to run machine learning model on large datasets. It is the process of generating outputs on a batch of observations.\n",
    "\n",
    "With batch inference, the batch runs are typically generated during some recurring schedule (e.g., hourly, or daily). These inferences are then stored in a database or a file and can be made available to developers or end users. With batch inference, the goal is usually tied to time constraints and the service-level agreement (SLA) of the job. Conversely, in real time serving, the goal is usually to optimize the number of transactions per second that the model can process. An online application displays a result to the user.\n",
    "\n",
    "Batch inference can sometimes take advantage of big data technologies, such as Spark, to generate predictions. Big data technologies allow data scientists and machine learning engineers to take advantage of scalable compute resources to generate many predictions simultaneously. To gain a better understanding about the batch inference usage and the function parameters, see the [Batch Inference page on the Function Hub](https://www.mlrun.org/hub/functions/master/batch_inference_v2/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test your model\n",
    "\n",
    "To evaluate batch model prior to deployment, you should use the `evaluate` handler of the `auto_trainer` function.\n",
    "\n",
    "This is typically done during model development. For more information refer to the {ref}`auto_trainer_evaluate` handler documentation. For example:\n",
    "``` python\n",
    "import mlrun\n",
    "\n",
    "# Set the base project name\n",
    "project_name_base = 'batch-inference'\n",
    "\n",
    "# Initialize the MLRun project object\n",
    "project = mlrun.get_or_create_project(project_name_base, context=\"./\", user_project=True)\n",
    "\n",
    "auto_trainer = project.set_function(mlrun.import_function(\"hub://auto_trainer\"))\n",
    "\n",
    "evaluate_run = project.run_function(\n",
    "    auto_trainer,\n",
    "    handler=\"evaluate\",\n",
    "    inputs={\"dataset\": train_run.outputs['test_set']},\n",
    "    params={\n",
    "        \"model\": train_run.outputs['model'],\n",
    "        \"label_columns\": \"labels\",\n",
    "    },\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy your model\n",
    "\n",
    "Batch inference is implemented in MLRun by running the function with an input dataset. With MLRun you can easily create any custom logic in a function, including loading a model and calling it.\n",
    "\n",
    "The Function Hub [batch inference function](https://github.com/mlrun/functions/tree/master/batch_inference_v2) is used for running the models in batch as well as performing drift analysis. The function supports the following frameworks:\n",
    "\n",
    "- Scikit-learn\n",
    "- XGBoost\n",
    "- LightGBM  \n",
    "- Tensorflow/Keras\n",
    "- PyTorch\n",
    "- ONNX\n",
    "\n",
    "Internally the function uses MLRun's out-of-the-box capability to load run a model via the {py:class}`mlrun.frameworks.auto_mlrun.auto_mlrun.AutoMLRun` class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic example\n",
    "The simplest example to run the function is as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create project\n",
    "\n",
    "Import MLRun and create a project:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "source": [
    "import mlrun\n",
    "\n",
    "project = mlrun.get_or_create_project(\n",
    "    \"batch-inference\", context=\"./\", user_project=True\n",
    ")\n",
    "batch_inference = mlrun.import_function(\"hub://batch_inference_v2\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get the model\n",
    "\n",
    "Get the model. The model is a [decision tree classifier](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html) from scikit-learn. Note that if you previously trained your model using MLRun, you can reference the model artifact produced during that training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "source": [
    "import sys\n",
    "\n",
    "suffix = (\n",
    "    mlrun.__version__.split(\"-\")[0].replace(\".\", \"_\")\n",
    "    if sys.version_info[1] > 7\n",
    "    else \"3.7\"\n",
    ")\n",
    "\n",
    "model_path = mlrun.get_sample_path(f\"models/batch-predict/model-{suffix}.pkl\")\n",
    "\n",
    "model_artifact = project.log_model(\n",
    "    key=\"model\", model_file=model_path, framework=\"sklearn\"\n",
    ")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get the data\n",
    "\n",
    "Get the dataset to perform the inference. The dataset is in `parquet` format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "source": [
    "prediction_set_path = mlrun.get_sample_path(\"data/batch-predict/prediction_set.parquet\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run the batch inference function    \n",
    "\n",
    "Run the inference. In the first example we will not perform any drift analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "source": [
    "batch_run = project.run_function(\n",
    "    batch_inference,\n",
    "    inputs={\"dataset\": prediction_set_path, \"model_path\": model_artifact.uri},\n",
    ")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function output\n",
    "\n",
    "The output of the function is an artifact called `prediction`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "source": [
    "batch_run.artifact(\"prediction\").as_df().head()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### View the results in the UI\n",
    "\n",
    "The output is saved as a parquet file under the project artifact path. In the UI you can go to the `batch-inference-infer` job --> artifact tab to view the details.\n",
    "\n",
    "![batch prediction results](../_static/images/batch_inference_prediction_artifact.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scheduling a batch run\n",
    "\n",
    "To schedule a run, you can set the schedule parameter of the run method. The scheduling is done by using a cron format.\n",
    "\n",
    "You can also schedule runs from the dashboard. On the Projects > Jobs and Workflows page, you can create a new job using the New Job wizard. At the end of the wizard, you can set the job scheduling. In the following example, the job is set to run every 30 minutes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "source": [
    "batch_run = project.run_function(\n",
    "    batch_inference,\n",
    "    inputs={\"dataset\": prediction_set_path, \"model_path\": model_artifact.uri},\n",
    "    schedule=\"*/30 * * * *\",\n",
    ")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drift analysis\n",
    "\n",
    "By default, if a model has a sample set statistics, `batch_inference` performs drift analysis and produces a data drift table artifact, as well as numerical drift metrics. In addition, this function either creates or updates an existing [model endpoint](../monitoring/model-monitoring-deployment.html#:~:text=Model%20Endpoint%20%E2%80%94%20A%20combination%20of%20a%20deployed%20Nuclio%20function%20and%20the%20models%20themselves.%20One%20function%20can%20run%20multiple%20endpoints%3B%20however%2C%20statistics%20are%20saved%20per%20endpoint.) record (depends on the provided `endpoint_id`). \n",
    "\n",
    "In addition, you can define `\"trigger_monitoring_job\": True` to trigger the drift job analysis immediately. \n",
    "\n",
    "To provide sample set statistics for the model you can either:\n",
    "\n",
    "1. Train the model using MLRun. This allows you to create the sample set during training.\n",
    "2. Log an external model using `project.log_model` method and provide the training set in the `training_set` parameter.\n",
    "3. Provide the set explicitly when calling the `batch_inference` function via the `model_endpoint_sample_set` input.\n",
    "\n",
    "In the example below, we will provide the training set as the sample set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "source": [
    "training_set_path = mlrun.get_sample_path(\"data/batch-predict/training_set.parquet\")\n",
    "\n",
    "batch_run = project.run_function(\n",
    "    batch_inference,\n",
    "    inputs={\n",
    "        \"dataset\": prediction_set_path,\n",
    "        \"model_endpoint_sample_set\": training_set_path,\n",
    "        \"model_path\": model_artifact.uri,\n",
    "    },\n",
    "    params={\n",
    "        \"label_columns\": \"label\",\n",
    "        \"perform_drift_analysis\": True,\n",
    "        \"trigger_monitoring_job\": True,\n",
    "    },\n",
    ")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, instead of just prediction, you get drift analysis. If no label column was provided, the job function tries to retrieve the label columns from the logged model artifact. If also not defined in the model, the label columns are generated with the following format `predicted_label_{i}` where `i` is an incremental number. \n",
    "\n",
    "The drift table plot that compares the drift between the training data and prediction data per feature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "source": [
    "batch_run.artifact(\"drift_table_plot\").show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![batch inference drift table plot](../tutorials/_static/images/drift_table_plot.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You also get a numerical drift metric and boolean flag denoting whether or not data drift is detected:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "source": [
    "print(batch_run.status.results)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "source": [
    "# Data/concept drift per feature (use batch_run.artifact(\"features_drift_results\").get() to obtain the raw data)\n",
    "batch_run.artifact(\"features_drift_results\").show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
