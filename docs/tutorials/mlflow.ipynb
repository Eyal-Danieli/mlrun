{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c478ebb2",
   "metadata": {},
   "source": [
    "# MLflow tracker\n",
    "\n",
    "This tutorial demonstrates how to seamlessly integrate and transfer logs from MLflow to MLRun, </br>\n",
    "creating a unified and powerful platform for your machine learning experiments.\n",
    "\n",
    "You can combine MLflow and MLRun for a comprehensive solution for managing, tracking, and deploying machine learning models. \n",
    "\n",
    "This notebook guides you through the process of:\n",
    "\n",
    "1. Setting up the integration between MLflow and MLRun.\n",
    "2. Extracting data, metrics, and artifacts from MLflow experiments.\n",
    "3. Creating MLRun artifacts and projects to organize and manage the transferred data.\n",
    "4. Leveraging MLRun's capabilities for model deployment and data processing.\n",
    "\n",
    "By the end of this tutorial, you will have a understanding of how to establish a smooth flow of data between MLflow and MLRun.\n",
    "\n",
    "## MLRun installation and configuration\n",
    "Before running this notebook make sure the mlrun package is installed (pip install mlrun) and that you have configured the access to MLRun service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab49e1f1",
   "metadata": {},
   "source": [
    "# Install MLRun and scikit-learn if not already installed. Run this only once. Restart the notebook after the install!\n",
    "%pip install mlrun scikit-learn~=1.4.0 xgboost~=2.0.3"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "9a8db175-51f4-4218-afd1-752cc0e65216",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Create an MLflow Xgboost function\n",
    "\n",
    " The `training.py` contains just mlflow code, and does not have any dependence on MLRun."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "44a1e133-954d-47a3-9b0f-6e181fe12ea7",
   "metadata": {},
   "source": [
    "%%writefile training.py\n",
    "\n",
    "import mlflow\n",
    "import mlflow.xgboost\n",
    "import xgboost as xgb\n",
    "from mlflow import log_metric\n",
    "from sklearn import datasets\n",
    "from sklearn.metrics import accuracy_score, log_loss\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def example_xgb_run():\n",
    "    # Prepare, train, and test data\n",
    "    iris = datasets.load_iris()\n",
    "    X = iris.data\n",
    "    y = iris.target\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42\n",
    "    )\n",
    "\n",
    "    # Enable auto logging\n",
    "    mlflow.xgboost.autolog()\n",
    "\n",
    "    dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "    dtest = xgb.DMatrix(X_test, label=y_test)\n",
    "\n",
    "    with mlflow.start_run():\n",
    "        # Train model\n",
    "        params = {\n",
    "            \"objective\": \"multi:softprob\",\n",
    "            \"num_class\": 3,\n",
    "            \"learning_rate\": 0.3,\n",
    "            \"eval_metric\": \"mlogloss\",\n",
    "            \"colsample_bytree\": 1.0,\n",
    "            \"subsample\": 1.0,\n",
    "            \"seed\": 42,\n",
    "        }\n",
    "        model = xgb.train(params, dtrain, evals=[(dtrain, \"train\")])\n",
    "        \n",
    "        # Evaluate model\n",
    "        y_proba = model.predict(dtest)\n",
    "        y_pred = y_proba.argmax(axis=1)\n",
    "        loss = log_loss(y_test, y_proba)\n",
    "        acc = accuracy_score(y_test, y_pred)\n",
    "        \n",
    "        # Log metrics by hand\n",
    "        mlflow.log_metrics({\"log_loss\": loss, \"accuracy\": acc})"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "1cf984c9-78a9-443f-9465-111263101dcd",
   "metadata": {},
   "source": [
    "## Log the data from MLflow in MLRun "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "365e4b39-9f39-40ae-aac4-7c4f42bce9bd",
   "metadata": {},
   "source": [
    "### Change the MLRun configuration to use the tracker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b194d04-e08f-4161-a65b-4f18d10fdbf0",
   "metadata": {},
   "source": [
    "import mlrun\n",
    "\n",
    "mlrun.mlconf.external_platform_tracking.enabled = True"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "b16bb4db-8a2a-4453-a42e-0e8e74ab8f53",
   "metadata": {},
   "source": [
    "To run the tracking, set:\n",
    "`mlrun.mlconf.external_platform_tracking.mlflow.match_experiment_to_runtime` to True.<br>\n",
    "This makes the MLRun run-id the same as the MLFlow experiment ID.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b7bc72a-bd1b-408a-afa8-e474d91c4a20",
   "metadata": {},
   "source": [
    "### Create the project and function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3382b909-a8dc-41a3-afb1-b64df9bb7318",
   "metadata": {},
   "source": [
    "# Set the tracking\n",
    "mlrun.mlconf.external_platform_tracking.mlflow.match_experiment_to_runtime = True\n",
    "\n",
    "# Create a project for this demo:\n",
    "project = mlrun.get_or_create_project(name=\"mlflow-tracking-example\", context=\"./\")\n",
    "\n",
    "# Create a MLRun function using the example train file (all the functions must be located in it):\n",
    "training_func = project.set_function(\n",
    "    func=\"training.py\",\n",
    "    name=\"example-xgb-run\",\n",
    "    kind=\"job\",\n",
    "    image=\"mlrun/mlrun\",\n",
    ")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "91597f57-364d-4d2a-b926-97b9d8afc81b",
   "metadata": {},
   "source": [
    "### Run the function\n",
    "\n",
    "After running the function, you can look at the UI and see that all metrics and parameters are logged in MLRun."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8ba452dd-1756-4bfb-af64-d741e234dba3",
   "metadata": {},
   "source": [
    "# Run the example code using mlrun\n",
    "train_run = training_func.run(\n",
    "    local=True,\n",
    "    handler=\"example_xgb_run\",\n",
    ")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "655d5c46-2c0a-46f2-bbec-a58853260476",
   "metadata": {},
   "source": [
    "### Examine the results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d23beb02-e455-48dc-9d9f-9e3d4549ec71",
   "metadata": {},
   "source": [
    "train_run.outputs"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b05f4c2a-5f2d-4d7c-9c21-39c0a949cfc3",
   "metadata": {},
   "source": [
    "train_run.status.results"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "925b3445-18b4-4497-9783-52b4cd069401",
   "metadata": {},
   "source": [
    "train_run.artifact(\"feature_importance_weight_png\").show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "227c4358-4c34-4d1c-acb4-e37ca110b8bf",
   "metadata": {},
   "source": [
    "### You can also examine the results using the UI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde00fd1-a1f0-4c56-80c2-c5d36a9062a1",
   "metadata": {},
   "source": [
    "Look at collected artifacts: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cda6c13-7fee-4284-aacf-81a506a426da",
   "metadata": {},
   "source": [
    "<img src=\"./_static/images/mlflow-artifacts.png\" width=\"1200\" >\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1525230-e10c-4f48-b951-bc73642bb3e4",
   "metadata": {},
   "source": [
    "And at results:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "217279f8-6af1-4209-b0ec-3d3d829ceed9",
   "metadata": {},
   "source": [
    "<img src=\"./_static/images/mlflow-results.png\" width=\"1200\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "844edc05-0b6a-4e84-9213-1d3cbf6f833e",
   "metadata": {},
   "source": [
    "## Use the function for model serving"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af6489cb-35e2-4b0d-8a0d-e8954a44534a",
   "metadata": {},
   "source": [
    "### Implement the load and predict functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "381bb171-5111-4cc1-b81c-fdb813a6208f",
   "metadata": {},
   "source": [
    "%%writefile serving.py\n",
    "\n",
    "import zipfile\n",
    "from typing import Any, Dict, List, Union\n",
    "\n",
    "import mlflow\n",
    "import numpy as np\n",
    "import os\n",
    "import mlrun\n",
    "from mlrun.serving.v2_serving import V2ModelServer\n",
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "\n",
    "class MLFlowModelServer(V2ModelServer):\n",
    "    \"\"\"\n",
    "    The MLFlow tracker Model serving class  inherits the V2ModelServer class, resulting in automatic \n",
    "    initialization by the model server. It can run locally as part of a nuclio serverless function,\n",
    "    or as part of a real-time pipeline.\n",
    "    \"\"\"\n",
    "\n",
    "    def load(self):\n",
    "        \"\"\"\n",
    "        loads a model that was logged by the MLFlow tracker model\n",
    "        \"\"\"\n",
    "        # Unzip the model dir and then use mlflow's load function\n",
    "        model_file, _ = self.get_model(\".zip\")\n",
    "        model_path_unzip = model_file.replace(\".zip\", \"\")\n",
    "\n",
    "        with zipfile.ZipFile(model_file, \"r\") as zip_ref:\n",
    "            zip_ref.extractall(model_path_unzip)\n",
    "            \n",
    "        self.model = mlflow.pyfunc.load_model(model_path_unzip)\n",
    "\n",
    "    def predict(self, request: Dict[str, Any]) -> list:\n",
    "        \"\"\"\n",
    "        Infer the inputs through the model. The inferred data\n",
    "        is read from the \"inputs\" key of the request.\n",
    "\n",
    "        :param request: The request to the model using xgboost's predict. \n",
    "                The input to the model is read from the \"inputs\" key.\n",
    "\n",
    "        :return: The model's prediction on the given input.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Get the inputs and set to accepted type:\n",
    "        inputs = pd.DataFrame(request[\"inputs\"])\n",
    "\n",
    "        # Predict using the model's predict function:\n",
    "        predictions = self.model.predict(inputs)\n",
    "\n",
    "        # Return as list:\n",
    "        return predictions.tolist()\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "40182a6f-fc46-4a33-a7f5-7ee8ee171966",
   "metadata": {},
   "source": [
    "### Create the server and serving function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f5fe910b-e177-4af7-84de-41a571d1774c",
   "metadata": {},
   "source": [
    "serving_func = project.set_function(\n",
    "    func=\"serving.py\",\n",
    "    name=\"example-xgb-server\",\n",
    "    kind=\"serving\",\n",
    "    image=\"mlrun/mlrun\",\n",
    ")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ddbfd48f-a90e-4fe6-9caa-ddffeacf63d1",
   "metadata": {},
   "source": [
    "# Add the model\n",
    "serving_func.add_model(\n",
    "    \"mlflow_xgb_model\",\n",
    "    class_name=\"MLFlowModelServer\",\n",
    "    model_path=train_run.outputs[\"model\"],\n",
    ")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2298d111-2f53-4b84-be9e-e4e8a228dcc4",
   "metadata": {},
   "source": [
    "# Create a mock server\n",
    "server = serving_func.to_mock_server()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "f54d7c06-4972-4881-9bc9-fba7db0adbe4",
   "metadata": {},
   "source": [
    "### Test the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4f256490-f225-4bd6-ac8a-5fc12a0f335d",
   "metadata": {},
   "source": [
    "# An example taken randomly from the dataset that the model was trained on, each\n",
    "x = [[5.1, 3.5, 1.4, 0.2]]\n",
    "result = server.test(\"/v2/models/mlflow_xgb_model/predict\", {\"inputs\": x})"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "47839f4b-bb2d-4341-99c5-e34fa31270c9",
   "metadata": {},
   "source": [
    "# Look at the result, it shows the probability of the given example to be each of the\n",
    "# irises featured in the dataset\n",
    "result"
   ],
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
