{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using MLRUN function locally, as a Kubernetes Job, and in a Workflow\n",
    "  --------------------------------------------------------------------\n",
    "\n",
    "#### **notebook how-to's**\n",
    "* Write and test code in a notebook.\n",
    "* Convert it to a containerized image.\n",
    "* Run it on a Kubernetes cluster with shared file or object storage.\n",
    "* Run it in an automated workflow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='top'></a>\n",
    "#### **steps**\n",
    "**[intall mlrun](#install)**<br>\n",
    "**[define a new function and its dependencies](#define-function)**<br>\n",
    "**[test the function code and pipeline locally](#test-locally)**<br>\n",
    "**[define cluster jobs and build images](#build)**<br>\n",
    "**[deploy (build) the function container](#deploy-build)**<br>\n",
    "**[run the function on the cluster](#run-on-cluster)**<br>\n",
    "**[create and run a KubeFlow Pipeline](#create-pipeline)**<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"install\" ></a>\n",
    "______________________________________________\n",
    "### **install mlrun**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "source": [
    "# Uncomment this to install mlrun package, restart the kernel after\n",
    "\n",
    "# !pip install -U mlrun"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "source": [
    "# set the UI external URL (will generate ui hyperlinks)\n",
    "# %env MLRUN_UI_URL=http://<mlrun-ui-url>:<port>"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "______________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='define-function'></a>\n",
    "### **define a new function and its dependencies**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "source": [
    "# mlrun: ignore\n",
    "# do not remove the comment above (it is a directive to nuclio, ignore that cell during build)\n",
    "# if the nuclio-jupyter package is not installed run !pip install nuclio-jupyter and restart the kernel\n",
    "import nuclio"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use `%nuclio` magic commands to set package dependencies and configuration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "source": [
    "%nuclio cmd -c pip install pandas\n",
    "%nuclio config spec.build.baseImage = \"mlrun/mlrun\""
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ```DataItem```s and the ```context``` within which they are logged are described in the following ```mlrun``` modules (they are included here only for type clarity)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "source": [
    "from mlrun.execution import MLClientCtx\n",
    "from mlrun.datastore import DataItem\n",
    "from mlrun.artifacts import get_model, update_model"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def training(context: MLClientCtx, p1: int = 1, p2: int = 2) -> None:\n",
    "    \"\"\"Train a model.\n",
    "\n",
    "    :param context: The runtime context object.\n",
    "    :param p1: A model parameter.\n",
    "    :param p2: Another model parameter.\n",
    "    \"\"\"\n",
    "    # access input metadata, values, and inputs\n",
    "    print(f\"Run: {context.name} (uid={context.uid})\")\n",
    "    print(f\"Params: p1={p1}, p2={p2}\")\n",
    "    context.logger.info(\"started training\")\n",
    "\n",
    "    # <insert training code here>\n",
    "\n",
    "    # log the run results (scalar values)\n",
    "    context.log_result(\"accuracy\", p1 * 2)\n",
    "    context.log_result(\"loss\", p1 * 3)\n",
    "\n",
    "    # add a lable/tag to this run\n",
    "    context.set_label(\"category\", \"tests\")\n",
    "\n",
    "    # log a simple artifact + label the artifact\n",
    "    # If you want to upload a local file to the artifact repo add src_path=<local-path>\n",
    "    context.log_artifact(\"somefile\", body=b\"abc is 123\", local_path=\"myfile.txt\")\n",
    "\n",
    "    # create a dataframe artifact\n",
    "    df = pd.DataFrame([{\"A\": 10, \"B\": 100}, {\"A\": 11, \"B\": 110}, {\"A\": 12, \"B\": 120}])\n",
    "    context.log_dataset(\"mydf\", df=df)\n",
    "\n",
    "    # Log an ML Model artifact, add metrics, params, and labels to it\n",
    "    # and place it in a subdir ('models') under artifacts path\n",
    "    context.log_model(\n",
    "        \"mymodel\",\n",
    "        body=b\"abc is 123\",\n",
    "        model_file=\"model.txt\",\n",
    "        metrics={\"accuracy\": 0.85},\n",
    "        parameters={\"xx\": \"abc\"},\n",
    "        labels={\"framework\": \"xgboost\"},\n",
    "        artifact_path=context.artifact_subpath(\"models\"),\n",
    "    )"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "source": [
    "def validation(context: MLClientCtx, model: DataItem) -> None:\n",
    "    \"\"\"Model validation.\n",
    "\n",
    "    Dummy validation function.\n",
    "\n",
    "    :param context: The runtime context object.\n",
    "    :param model: The extimated model object.\n",
    "    \"\"\"\n",
    "    # access input metadata, values, files, and secrets (passwords)\n",
    "    print(f\"Run: {context.name} (uid={context.uid})\")\n",
    "    context.logger.info(\"started validation\")\n",
    "\n",
    "    # get the model file, class (metadata), and extra_data (dict of key: DataItem)\n",
    "    model_file, model_obj, _ = get_model(model)\n",
    "\n",
    "    # update model object elements and data\n",
    "    update_model(model_obj, parameters={\"one_more\": 5})\n",
    "\n",
    "    print(f\"path to local copy of model file - {model_file}\")\n",
    "    print(\"parameters:\", model_obj.parameters)\n",
    "    print(\"metrics:\", model_obj.metrics)\n",
    "    context.log_artifact(\"validation\", body=b\"<b> validated </b>\", format=\"html\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following end-code annotation tells ```nuclio``` to stop parsing the notebook from this cell. _**Please do not remove this cell**_:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "source": [
    "# mlrun: end-code"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "______________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='test-locally'></a>\n",
    "### **test the function code and pipeline locally**\n",
    "The functions above can be tested locally. Parameters, inputs, and outputs can be specified in the API or the `Task` object.\n",
    "\n",
    "We create a ```function``` which defines the runtime environment (type, code, image, ..) and ```run()``` a job or experiments using that function.\n",
    "\n",
    "We use the ```local``` runtime by default, later on we will use a ```job``` runtime for running containers, and can use other distributed runners like MpiJob, Spark, Dask, and Nuclio.\n",
    "\n",
    "In each run we can specify the function, inputs, parameters/hyper-parameters, etc... For more details, see the [mlrun_basics notebook](mlrun_basics.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "source": [
    "from mlrun import run_local, code_to_function, mlconf, new_task\n",
    "from mlrun.platforms.other import auto_mount\n",
    "\n",
    "mlconf.dbpath = mlconf.dbpath or \"http://mlrun-api:8080\""
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> define the artifact location</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "source": [
    "from os import path\n",
    "\n",
    "out = mlconf.artifact_path or path.abspath(\"./data\")\n",
    "# {{run.uid}} will be substituted with the run id, so output will be written to different directoried per run\n",
    "artifact_path = path.join(out, \"{{run.uid}}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### _running and linking multiple tasks_\n",
    "In this example we run two functions, ```training``` and ```validation``` and we pass the result from one to the other.\n",
    "We will see in the ```job``` example that linking works even when the tasks are run in a workflow on different processes or containers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```run_local()``` will run our task on a local function:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the training function. Functions can have multiple handlers/methods, here we call the ```training``` handler:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "source": [
    "train_run = run_local(new_task(handler=training, params={\"p1\": 5}, artifact_path=out))"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the function runs it generates the result widget, you can click the `model` artifact to see its content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "source": [
    "train_run.outputs"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output from the first training function is passed to the validation function, let's run it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "source": [
    "model = train_run.outputs[\"mymodel\"]\n",
    "\n",
    "validation_run = run_local(\n",
    "    new_task(handler=validation, inputs={\"model\": model}, artifact_path=out)\n",
    ")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "______________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"build\"></a>\n",
    "### **define cluster jobs and build images**\n",
    "\n",
    "In order to use our function in a cluster we need to package our code and dependencies.\n",
    "\n",
    "The ```code_to_function``` call will automatically generate a ```function``` object from the current notebook (or a specified file) with its list of dependencies and runtime configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "source": [
    "# create an ML function from the notebook, attache it to iguazio data fabric (v3io)\n",
    "trainer = code_to_function(name=\"my-trainer\", kind=\"job\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The functions need shared storage (file or object) media to pass and store artifacts.\n",
    "\n",
    "You can add _**Kubernetes**_ resources like volumes, environment variables, secrets, cpu/mem/gpu, etc. to a function.\n",
    "\n",
    "```mlrun``` uses _**KubeFlow**_ modifiers (apply) to configure resources, you can build your own or use predefined ones e.g. for [AWS resources](https://github.com/kubeflow/pipelines/blob/master/sdk/python/kfp/aws.py).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### _**Option 1: Using file volumes for artifacts**_\n",
    "If your are using [Iguazio data science platform](https://www.iguazio.com/) use the `mount_v3io()` auto-mount modifier.<br>\n",
    "if you use other k8s PVC volumes you can use the `mlrun.platforms.mount_pvc(..)` modifier with the required params.\n",
    "\n",
    "We will use the `auto_mount()` modifier which auto selects between the options, you can set the PVC volume config via env var:\n",
    "```\n",
    "    MLRUN_PVC_MOUNT=<pvc-name>:<mount-path>\n",
    "```\n",
    "\n",
    "Applying ```mount_v3io()``` will attach the function to Iguazio's real-time data fabric (mounted by default to _**home**_ of the current user).\n",
    "\n",
    "**Note**: if the notebook is not on the managed platform (running remotely) you need to create and use a v3io secret, run:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`kubectl create -n <namespace> secret generic my-v3io --from-literal=accessKey=<your access key> --from-literal=username=<your user name> --type v3io/fuse`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and use: `trainer.apply(mount_v3io(user='admin', secret='my-v3io'))`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So for our current ```training``` function, when using Iguazio data science platform run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "source": [
    "# for auto choice between Iguazio platform and k8s PVC\n",
    "# should set the env var for PVC: MLRUN_PVC_MOUNT=<pvc-name>:<mount-path>, or use mount_pvc()\n",
    "trainer.apply(auto_mount())"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "source": [
    "# Uncomment for use with shared PVC volumes, e.g. using the NFS Share in the local k8s install\n",
    "# from mlrun.platforms import mount_pvc\n",
    "# trainer.apply(mount_pvc('nfsvol', 'nfsvol', '/home/jovyan/data'))"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### _**Option 2: Using AWS S3 for artifacts**_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In AWS you can use S3 and need to have a `secret` with AWS credentials. An AWS secret can be created with the following command line:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`kubectl create -n <namespace> secret generic my-aws --from-literal=AWS_ACCESS_KEY_ID=<access key> --from-literal=AWS_SECRET_ACCESS_KEY=<secret key>`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use the secret:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "source": [
    "# from kfp.aws import use_aws_secret"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "source": [
    "# trainer.apply(use_aws_secret(secret_name='my-aws'))\n",
    "# out = 's3://<your-bucket-name>/jobs/{{run.uid}}'"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "______________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"deploy-build\"></a>\n",
    "### **deploy (build) the function container**\n",
    "\n",
    "The `deploy()` command will build a custom container image (create a cluster build job) from the outlined function dependencies.\n",
    "\n",
    "If a pre-built container image already exists, pass the `image` name instead. _**Note that the code and params can be updated per run without building a new image**_.\n",
    "\n",
    "The image is stored in a container repository, and by default it uses the repository configured on the MLRun API service, you can specify your own docker registry by first creating a secret, and adding that secret name to the build configuration:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`kubectl create -n <namespace> secret docker-registry my-docker --docker-server=https://index.docker.io/v1/ --docker-username=<your-user> --docker-password=<your-password> --docker-email=<your-email>`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and run this: `trainer.build_config(image='target/image:tag', secret='my_docker')`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "trainer.deploy(with_mlrun=False)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"run-on-cluster\"></a>\n",
    "### **run the function on the cluster**\n",
    "\n",
    "\n",
    "In case we made changes to the code, ```with_code``` will inject the latest code into the function (it doesn't require a new build)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "source": [
    "trainer.with_code()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "source": [
    "# create the base task (common to both steps), and set the output path and experiment label\n",
    "base_task = new_task(artifact_path=out).set_label(\"stage\", \"dev\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "source": [
    "# run our training task, with hyper params, and select the one with max accuracy\n",
    "train_task = new_task(\n",
    "    name=\"my-training\", handler=\"training\", params={\"p1\": 9}, base=base_task\n",
    ")\n",
    "train_run = trainer.run(train_task)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "source": [
    "# running validation, use the model result from the previous step\n",
    "model = train_run.outputs[\"mymodel\"]\n",
    "trainer.run(base_task, handler=\"validation\", inputs={\"model\": model}, watch=True)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "______________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"create-pipeline\"></a>\n",
    "### **create and run a KubeFlow pipeline**\n",
    "\n",
    "KubeFlow pipelines are used for workflow automation--we compose a graph of functions and specify parameters, inputs and outputs.\n",
    "\n",
    "As illustrated below, we can chain the outputs and inputs of the pipeline steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "source": [
    "import kfp\n",
    "from kfp import dsl\n",
    "from mlrun import run_pipeline"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "source": [
    "@dsl.pipeline(name=\"job test\", description=\"demonstrating mlrun usage\")\n",
    "def job_pipeline(p1: int = 9) -> None:\n",
    "    \"\"\"Define our pipeline.\n",
    "\n",
    "    :param p1: A model parameter.\n",
    "    \"\"\"\n",
    "\n",
    "    train = trainer.as_step(handler=\"training\", params={\"p1\": p1}, outputs=[\"mymodel\"])\n",
    "\n",
    "    validate = trainer.as_step(\n",
    "        handler=\"validation\",\n",
    "        inputs={\"model\": train.outputs[\"mymodel\"]},\n",
    "        outputs=[\"validation\"],\n",
    "    )"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The job pipeline can compiled to a yaml file that can be used for debugging:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "source": [
    "kfp.compiler.Compiler().compile(job_pipeline, \"jobpipe.yaml\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### running the pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pipeline results are stored at the `artifact_path` location:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, by adding ```/{{workflow.uid}}``` to the path ```mlrun``` will generate a unique folder per workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "source": [
    "artifact_path = \"v3io:///users/admin/kfp/{{workflow.uid}}/\""
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "source": [
    "arguments = {\"p1\": 8}\n",
    "run_id = run_pipeline(\n",
    "    job_pipeline, arguments, experiment=\"my-job\", artifact_path=artifact_path\n",
    ")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[top](#top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "source": [
    "from mlrun import wait_for_pipeline_completion, get_run_db\n",
    "\n",
    "wait_for_pipeline_completion(run_id)\n",
    "db = get_run_db().connect()\n",
    "db.list_runs(project=\"default\", labels=f\"workflow={run_id}\").show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [],
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
