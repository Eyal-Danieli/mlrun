{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using MLRUN with Dask Distributed Jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "source": [
    "# recommended, installing the exact package versions as we use in the worker\n",
    "#!pip install dask==2.12.0 distributed==2.14.0"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing a function code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "source": [
    "# function that will be distributed\n",
    "def inc(x):\n",
    "    return x + 2"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MLRun context in the case of Dask will have an extra param `dask_client`\n",
    "which is initialized based on the function spec (below), and can be used to submit Dask commands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "source": [
    "# wrapper function, uses the dask client object\n",
    "def hndlr(context, x=1, y=2):\n",
    "    context.logger.info(\"params: x={},y={}\".format(x, y))\n",
    "    print(\"params: x={},y={}\".format(x, y))\n",
    "    x = context.dask_client.submit(inc, x)\n",
    "    print(x)\n",
    "    print(x.result())\n",
    "    context.log_result(\"y\", x.result())"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "source": [
    "# mlrun: end-code\n",
    "# marks the end of a code section, do not delete"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "source": [
    "from mlrun import new_function, mlconf, code_to_function, mount_v3io, new_task\n",
    "\n",
    "# mlconf.dbpath = 'http://mlrun-api:8080'"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a Dask function object\n",
    "dask functions can be local (local workers), or remote (use containers in the cluster),\n",
    "in the case of `remote` users can specify the number of replica (optional) or leave blank for auto-scale.\n",
    "\n",
    "We use `code_to_function()` which packs the function code into the function object/yaml (eliminate the need to update the function image), we can use `new_function()` if the function code is part of the image or can be remotely mounted (e.g. via v3io mount).\n",
    "\n",
    "Dask function spec have several unique attributes:\n",
    "* **.remote** - bool, use local or clustered dask\n",
    "* **.replicas** - number of desired replicas, keep 0 for auto-scale\n",
    "* **.min_replicas, .max_replicas** - set replicas range for auto-scale\n",
    "* **.scheduler_timeout** - cluster will be killed after timeout (inactivity), default is `'60 minutes'`\n",
    "* **.nthreads** - number of worker threads\n",
    "* **.kfp_image** - optional, container image to use by KFP Pipeline runner (default to mlrun/dask)\n",
    "\n",
    "If you want to access the dask dashboard or scheduler from remote you need to use `NodePort` service type (set **.service_type** to 'NodePort'), and the external IP need to be specified in mlrun configuration (`mlconf.remote_host`), this will be set automatically if you are running on an Iguazio cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "source": [
    "# create the function from the notebook code + annotations, add volumes\n",
    "dsf = code_to_function(\"mydask\", kind=\"dask\").apply(mount_v3io())"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "source": [
    "dsf.spec.image = \"mlrun/ml-models\"\n",
    "dsf.spec.remote = True\n",
    "dsf.spec.replicas = 1\n",
    "dsf.spec.service_type = \"NodePort\""
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the function with extra packages\n",
    "We can skip the build section if we don't add packages (instead need to specify the image e.g. `dsf.spec.image='mlrun/ml-models'` which contains most of the packages you may need) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "source": [
    "# uncomment if you want to add packages to the workers and build a new image\n",
    "# dsf.build_config(base_image='mlrun/ml-models', commands=['pip install pandas'])\n",
    "# dsf.deploy()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run a task using our distributed dask function (cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "source": [
    "myrun = dsf.run(handler=hndlr, params={\"x\": 12})"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "source": [
    "# get the function status and addresses\n",
    "dsf.status.to_dict()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accessing the dask client directly\n",
    "You can get the dask client object and cluster information by reading the client object.\n",
    "\n",
    "> Note: the cluster can timeout, when you call the client MLRun will also verify the cluster is live and active and if not it will restart the dask cluster and refresh the function object with the latest addresses and status."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "source": [
    "c = dsf.client"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Access a dask function using the DB\n",
    "If we want to access the dask function (or its cluster), we can load the function object from the DB (assuming we already .run() or .save() it).\n",
    "\n",
    "This can be useful if we want to load the same function in a different notebook or container, or if we restarted our notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "source": [
    "from mlrun import import_function\n",
    "\n",
    "# Functions url: db://<project>/<name>[:tag]\n",
    "dsf_obj = import_function(\"db://default/mydask\")\n",
    "c = dsf_obj.client"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Pipeline using dask functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "source": [
    "import kfp\n",
    "from kfp import dsl\n",
    "from mlrun import run_pipeline"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "source": [
    "@dsl.pipeline(name=\"dask_pipeline\")\n",
    "def dask_pipe(x=1, y=10):\n",
    "    # use_db option will use a function (DB) pointer instead of adding the function spec to the YAML\n",
    "    myrun = dsf.as_step(\n",
    "        new_task(handler=hndlr, name=\"dask_pipeline\", params={\"x\": x, \"y\": y}),\n",
    "        use_db=True,\n",
    "    )\n",
    "\n",
    "    # if the step (dask client) need v3io access u should add: .apply(mount_v3io())\n",
    "\n",
    "    # if its a new image we may want to tell Kubeflow to reload the image\n",
    "    # myrun.container.set_image_pull_policy('Always')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "source": [
    "# for pipeline debug\n",
    "kfp.compiler.Compiler().compile(dask_pipe, \"daskpipe.yaml\", type_check=False)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "source": [
    "arguments = {\"x\": 4, \"y\": -5}\n",
    "artifact_path = \"/User/test\"\n",
    "run_id = run_pipeline(\n",
    "    dask_pipe,\n",
    "    arguments,\n",
    "    artifact_path=artifact_path,\n",
    "    run=\"DaskExamplePipeline\",\n",
    "    experiment=\"dask pipe\",\n",
    ")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "from mlrun import wait_for_pipeline_completion, get_run_db\n",
    "\n",
    "wait_for_pipeline_completion(run_id)\n",
    "db = get_run_db().connect()\n",
    "db.list_runs(project=\"default\", labels=f\"workflow={run_id}\").show()"
   ],
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
