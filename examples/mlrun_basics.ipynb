{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Started with MLRUN\n",
    "----------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='top'></a>\n",
    "### **Understanding functions and running tasks locally**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[intall mlrun](#install)**<br>\n",
    "**[mlrun setup](#setup)**<br>\n",
    "**[create and run a local function](#create-local)**<br>\n",
    "**[create a new mlrun Task and run it](#create-new-task)**<br>\n",
    "**[inspecting the run results and outputs](#inspecting)**<br>\n",
    "**[using hyperparameter tasks](#using-hyperparamter-tasks)**<br>\n",
    "**[running Task's through the cli](#tasks-cli)**<br>\n",
    "**[inline code and running on multiple runtimes](#inline)**<br>\n",
    "**[running locally in the notebook](#run-locally)**<br>\n",
    "**[hyper parameters taken from a csv file](#run-csv)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"install\" ></a>\n",
    "______________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **install**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "source": [
    "# Uncomment this to install mlrun package, restart the kernel after\n",
    "\n",
    "# !pip install mlrun"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"setup\"></a>\n",
    "______________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **mlrun setup**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MLRun tracks jobs and artifacts, collecting metadata in local file directory or in a DB.\n",
    "\n",
    "The DB/API path can be set using the environment variable ```MLRUN_DBPATH``` or the config object ```mlconf.dbpath```, we will try and get it from the environment.\n",
    "\n",
    "**Note:** for _distributed jobs_ and and an _interactive UI_ you must use the `mlrun-api` service (and not the file DB)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a local file DB, in the current folder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "source": [
    "from mlrun import run_local, new_task, mlconf\n",
    "from os import path\n",
    "\n",
    "mlconf.dbpath = mlconf.dbpath or \"./\""
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the ```mlrun-api``` service (in Kubernetes) use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "source": [
    "# uncomment for working with the DB\n",
    "# mlconf.dbpath = mlconf.dbpath or 'http://mlrun-api:8080'"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"create-local\"></a>\n",
    "______________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **create a new mlrun task and submit it to a local function**\n",
    "\n",
    "An mlrun ```Task``` defines job inputs/outputs and metadata:\n",
    "* input parameters\n",
    "* hyper-parameters (or parameter files)\n",
    "* input datasets\n",
    "* default paths (for input/output)\n",
    "* secrets (job credentials)\n",
    "* ```Task``` metadata: name, project, labels, etc.\n",
    "\n",
    "If a function supports multiple handlers (another term for method, function), we also need to define the specific handler (or set the `spec.default_handler`).\n",
    "\n",
    "`Task` object have helper methods like `.with_params()`, `.with_secrets()`, `.with_input()`, `.set_label()` for conviniance.\n",
    "\n",
    "This example shows how we can create a new task with various parameters and later use `.run()` to submit the task to our new function.<br>\n",
    "\n",
    "artifacts from each run are stored in the `artifact_path` which can be set globally through environment var (`MLRUN_ARTIFACT_PATH`) or through the config, if its not already set we can create a directory and use it in our runs. Using `{{run.uid}}` in the path will allow us to create a unique directory per run, when we use pipelines we can use the `{{workflow.uid}}` template option.\n",
    "\n",
    "> Note: artifact path can be a local path or a URL (starts with s3://, v3io://, etc.), if we want the artifacts to show in the UI the artifact path must be on a shared file or object media and should not be a relative path, on Iguazio platform the notebooks are always on the shared file system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "source": [
    "out = mlconf.artifact_path or path.abspath(\"./data\")\n",
    "# {{run.uid}} will be substituted with the run id, so output will be written to different directoried per run\n",
    "artifact_path = path.join(out, \"{{run.uid}}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we create a new task and set its properties using helper methods:<br>\n",
    "The [secrets `file`](secrets.txt) is a list of key=value properties "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "source": [
    "task = (\n",
    "    new_task(name=\"demo\", params={\"p1\": 5}, artifact_path=artifact_path)\n",
    "    .with_secrets(\"file\", \"secrets.txt\")\n",
    "    .set_label(\"type\", \"demo\")\n",
    ")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"create-new-task\"></a>\n",
    "______________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **run local code**\n",
    "The following example creates a temp local function mapped to the **[training.py](training.py)** code file (located in the same folder as this notebook) and run it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mlrun supports multiple _**runtimes**_ (handler, local, nuclio, job, spark, mpi, etc., see **[supported runtimes](https://github.com/mlrun/mlrun/tree/master/mlrun/runtimes)** for more details). _**local**_ runtime runs code in your local/notebook environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "source": [
    "# run our task using our new function\n",
    "run_object = run_local(task, command=\"training.py\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Hover over the inputs/artifacts to see full link, or click to see the content !!!</b>\n",
    "<a id=\"inspecting\"></a>\n",
    "______________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **inspecting the run results and outputs**\n",
    "\n",
    "Every ```run``` object (the result of a `.run()` method) has the following properties and methods:\n",
    "* `.uid()`   - return the unique id\n",
    "* `.state()` - return the last known state\n",
    "* `.show()`  - show the latest task state and data in a visual widget (with hyper links and hints)\n",
    "* `.outputs` - return a dict of the run results and artifact paths\n",
    "* `.logs()`  - return the latest logs, use `Watch=False` to disable interactive mode in running tasks\n",
    "* `.artifact(key)` - return full artifact details\n",
    "* `.output(key)`   - return specific result or artifact (path)\n",
    "* `.to_dict()`, `.to_yaml()`, `.to_json()` - convert the run object to dict/yaml/json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "source": [
    "run_object.uid()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "source": [
    "run_object.to_dict()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "source": [
    "run_object.state()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "source": [
    "run_object.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "source": [
    "run_object.outputs"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "source": [
    "run_object.logs()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "source": [
    "run_object.artifact(\"dataset\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"using-hyperparamter-tasks\"></a>\n",
    "______________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **using hyper-parameter tasks**\n",
    "In many cases we want to run the same function with different input values and select the best result.<br>\n",
    "\n",
    "You can specify parameters with a list of values and mlrun will run all the parameter combinations as a single hyper-param task.<br>\n",
    "\n",
    "Each unique run combination is called an _**iteration**_, where '0' iteration is the parent task.\n",
    "\n",
    "Use `.with_hyper_params()` and provide lists or values, we use the selector string to indicate which will iteration will be selected as the winning result (indicated using [```min``` or ```max```].[```output-value```])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "source": [
    "run = run_local(\n",
    "    task.with_hyper_params({\"p2\": [5, 2, 3]}, \"min.loss\"), command=\"training.py\"\n",
    ")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "source": [
    "run.outputs"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"tasks-cli\"></a>\n",
    "______________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **running tasks through the cli**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "source": [
    "%env MLRUN_DBPATH={mlconf.dbpath}"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "source": [
    "!mlrun run --name train_hyper -x p1=\"[3,7,5]\" --selector max.accuracy -p p2=5 --out-path {artifact_path} training.py\n",
    "if _exit_code != 0:\n",
    "    raise RuntimeError()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "source": [
    "# see other CLI commands\n",
    "!mlrun\n",
    "if _exit_code != 0:\n",
    "    raise RuntimeError()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"inline\"></a>\n",
    "______________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **using (inline) code and running on different runtimes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "source": [
    "from mlrun.artifacts import ChartArtifact, PlotArtifact\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# define a function with spec as parameter\n",
    "import time\n",
    "\n",
    "\n",
    "def handler(context, p1=1, p2=\"xx\"):\n",
    "    \"\"\"this is a simple function\n",
    "\n",
    "    :param p1:  first param\n",
    "    :param p2:  another param\n",
    "    \"\"\"\n",
    "    # access input metadata, values, and inputs\n",
    "    print(f\"Run: {context.name} (uid={context.uid})\")\n",
    "    print(f\"Params: p1={p1}, p2={p2}\")\n",
    "\n",
    "    time.sleep(1)\n",
    "\n",
    "    # log the run results (scalar values)\n",
    "    context.log_result(\"accuracy\", p1 * 2)\n",
    "    context.log_result(\"loss\", p1 * 3)\n",
    "\n",
    "    # add a lable/tag to this run\n",
    "    context.set_label(\"category\", \"tests\")\n",
    "\n",
    "    # create a matplot figure and store as artifact\n",
    "    fig, ax = plt.subplots()\n",
    "    np.random.seed(0)\n",
    "    x, y = np.random.normal(size=(2, 200))\n",
    "    color, size = np.random.random((2, 200))\n",
    "    ax.scatter(x, y, c=color, s=500 * size, alpha=0.3)\n",
    "    ax.grid(color=\"lightgray\", alpha=0.7)\n",
    "\n",
    "    context.log_artifact(PlotArtifact(\"myfig\", body=fig, title=\"my plot\"))\n",
    "\n",
    "    # create a dataframe artifact\n",
    "    df = pd.DataFrame([{\"A\": 10, \"B\": 100}, {\"A\": 11, \"B\": 110}, {\"A\": 12, \"B\": 120}])\n",
    "    context.log_dataset(\"mydf\", df=df)\n",
    "\n",
    "    # Log an ML Model artifact\n",
    "    context.log_model(\n",
    "        \"mymodel\",\n",
    "        body=b\"abc is 123\",\n",
    "        model_file=\"model.txt\",\n",
    "        model_dir=\"data\",\n",
    "        metrics={\"accuracy\": 0.85},\n",
    "        parameters={\"xx\": \"abc\"},\n",
    "        labels={\"framework\": \"xgboost\"},\n",
    "    )\n",
    "\n",
    "    return \"my resp\""
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"run-locally\"></a>\n",
    "______________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **run locally in the notebook**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "source": [
    "task = new_task(name=\"demo2\", handler=handler, artifact_path=artifact_path).with_params(\n",
    "    p1=7\n",
    ")\n",
    "run = run_local(task)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"run-csv\"></a>\n",
    "______________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **run with hyper parameters taken from a csv file**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "source": [
    "task = new_task(\n",
    "    name=\"demo2\", handler=handler, artifact_path=artifact_path\n",
    ").with_param_file(\"params.csv\", \"max.accuracy\")\n",
    "run = run_local(task)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [],
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
